{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10893751,"sourceType":"datasetVersion","datasetId":6770004},{"sourceId":11115228,"sourceType":"datasetVersion","datasetId":6930425}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width: 100%; clear: both;\">\n<div style=\"float: left; width: 50%;\">\n<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n</div>\n<div style=\"float: right; width: 50%;\">\n<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.975 · Deep Learning · PAC1</p>\n<p style=\"margin: 0; text-align:right;\">2024-2 · Màster universitari en Ciència de dades (Data science)</p>\n<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis de Informàtica, Multimèdia y Telecomunicació</p>\n</div>\n</div>\n<div style=\"width:100%;\">&nbsp;</div>\n\n# PEC 1: Xarxes neuronals artificials i convolucionals amb Keras - Classificació d’imatges i superresolució\n\nAl llarg d'aquesta pràctica, implementarem diversos models de xarxes neuronals, utilitzant Keras i la base de dades Imagenette (versió 320px). Concretament, abordarem les tasques següents:\n1. Descàrrega, anàlisi i pre-processament de les dades (1,5 pts)\n2. Xarxa neuronal artificial (ANN) completament connectada (1,5 pts)\n3. Petita xarxa neuronal convolucional (CNN) (2 pts)\n4. Augmentació de dades (1 pt)\n5. Superresolució d’imatges amb CNNs (1,5 pts)\n6. Inferència, mètriques i visualització dels resultats (1 pt)\n7. Model Pre-entrenat en Superresolució (1 pt)\n8. Comparació i conclusions (0,5 pts)\n\n<u> Consideracions generals </u>:\n\n- La solució proposada no pot utilitzar mètodes, funcions o paràmetres declarats **_deprecated_** en versions futures.\n- Aquest PAC s’ha de fer d’una manera **estrictament individual**. Qualsevol indici de còpia serà penalitzat amb un suspens (D) per a totes les parts implicades i la possible avaluació negativa de la totalitat de l'assignatura.\n- Cal que l'estudiant indiqui **totes les fonts** que ha utilitzat per a la realització del PEC. En cas contrari, es considerarà que l’alumne ha comès el plagi, sent penalitzat amb un suspens (D) i la possible avaluació negativa de la totalitat de l'assignatura.\n- Si s’utilitza qualsevol **IA generativa** en la resolució de la PAC **s’ha de referenciar** en aquelles seccions on s’ha utilitzat, com qualsevol altra font.\n\n<u>Format del lliurament </u>:\n\n- Alguns exercicis poden suposar diversos minuts d’execució, de manera que el lliurament s’ha de fer en format **Notebook** i en el format **html**, on es vegi el codi, els resultats i els comentaris de cada exercici. Podeu exportar el quadern a HTML a Jupyter Notebook des del menú File $\\to$ Download as $\\to$ HTML.\n- Hi ha un tipus especial de cel·la per allotjar el text. Aquest tipus de cel·les serà molt útil per respondre a les diferents preguntes teòriques plantejades al llarg de l’activitat. Per canviar el tipus de cel·la a aquest tipus, al menú: Cell $\\to$ Cell Type $\\to$ Markdown.","metadata":{"id":"QiqrZa4FjTln"}},{"cell_type":"markdown","source":"## 0. Context i càrrega de llibreries\nAl llarg d’aquesta pràctica, implementarem diversos models de xarxes neuronals per classificar les imatges de la base de dades [Imagenette](https://github.com/fastai/imagenette).\n\nLa base de dades Imagenette és un subconjunt de 10 classes fàcilment classificables de [Imagenet](https://www.image-net.org/), un projecte fonamental per avançar en la investigació sobre visió artificial i aprenentatge profund. Imagenette conté unes 13.000 imatges de diferents mides pertanyents a 10 categories (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute), cadascuna en una carpeta diferent.\n\nConcretament en aquesta PAC, utilitzarem una versió (Imagenette2-320) que ha estat re-escalada, però mantenint la relació d’aspecte de cada imatge (s’han ajustat de manera que la dimensió menor de cada imatge sigui de 320 píxels). Això atenuarà la càrrega computacional dels algoritmes quan s’utilitzen bases de dades d’imatges, però mantenint una qualitat suficient necessària per als nostres experiments. Les dades venen separades en 2 conjunts, entrenament i validació.\n\n**Nota: A causa de l’ús d’imatges com dades d’aquesta pràctica, l’entrenament de cada exercici es pot retardar entre uns minuts i més de mitja hora mitjançant GPU (els temps utilitzant CPU són significativament més llargs). Es recomana realitzar la pràctica en l’entorn que ofereix la plataforma Kaggle, ja que ofereix un entorn gratuït amb 30 hores setmanals per a l’ús de la GPU.**\n\nAl llarg de tota la pràctica, per a la creació de les diferents xarxes, alternem l’ús del model [Sequential](https://keras.io/guides/sequential_model/) i el model [Functional](https://keras.io/guides/functional_api/) de Keras a través de les seves classes [Sequential](https://keras.io/api/models/sequential/) i [Model](https://keras.io/api/models/model/) respectivament.\n\nEs recomana la lectura detallada de la documentació dels dos models per dur a terme la realització de la pràctica.\n\nComencem per instal·lar i carregar les llibreries més rellevants:","metadata":{"id":"xZW_y0x-jTlr"}},{"cell_type":"code","source":"# Instal·lem la darrera versió de Tensorflow (amb CUDA)\n%pip install tensorflow[and-cuda]","metadata":{"trusted":true,"id":"fpC54c2djTls","execution":{"iopub.status.busy":"2025-03-25T10:19:20.691306Z","iopub.execute_input":"2025-03-25T10:19:20.691604Z","iopub.status.idle":"2025-03-25T10:20:34.238084Z","shell.execute_reply.started":"2025-03-25T10:19:20.691583Z","shell.execute_reply":"2025-03-25T10:20:34.236957Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.37.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.26.4)\nCollecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow[and-cuda]) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow[and-cuda]) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow[and-cuda]) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow[and-cuda]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow[and-cuda]) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow[and-cuda]) (0.1.2)\nDownloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvcc-cu12\n    Found existing installation: nvidia-cuda-nvcc-cu12 12.6.85\n    Uninstalling nvidia-cuda-nvcc-cu12-12.6.85:\n      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.6.85\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Importem Tensorflow\nimport tensorflow as tf\nprint(\"TensorFlow version   : \", tf.__version__)\n\n# Necessitarem GPU\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n\n# Keras versió is 3.5.0\nfrom tensorflow import keras\nprint(\"Keras version   : \", keras.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"r8y4L-MWjTlt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importem els elements de Keras que utilitzarem amb més freqüència\nfrom keras.utils import image_dataset_from_directory\nfrom keras.layers import (Input, Dense, Dropout, Flatten, Conv2D, Conv2DTranspose,\n                          MaxPooling2D, UpSampling2D, Rescaling, Resizing,\n                          RandomFlip, RandomRotation)\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.optimizers import Adam","metadata":{"trusted":true,"id":"2L3kwGpbjTlu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importem algunes llibreries que necessitarem per a la PAC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pathlib import Path","metadata":{"trusted":true,"id":"8rfYpHJhjTlu"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Descàrrega, anàlisi i dades pre-processades (1,5 punts)\n\nEn aquesta secció explorarem la base de dades i prepararem la càrrega de les imatges per als models de les seccions següents.\n\nPer crear la nostra base de dades, hem de descarregar el fitxer d’imatges del següent [enllaç](https://drive.google.com/file/d/16kqYxjXK0xDDYQ4xF3PBgsa0NG3gwT4L/view?usp=sharing) (és un fitxer .zip que ocupa aproximadament 340 Mb).\n\n**Nota: per descarregar el fitxer d'imatges, heu de iniciar la sessió amb l'usuari i la contrasenya de la UOC.**\n\nA partir d’aquí:\n\n- Si treballem en local, simplement hem de descomprimir el fitxer descarregat.\n\n- Si treballem des de Kaggle hem de pujar el notebook de l'enunciat a la plataforma (per això podeu seguir els 6 primers passos del següent [article](https://rajputankit22.medium.com/how-to-upload-my-own-notebook-to-kaggle-2b0dedbb5a6b)) i després, un cop pujat el notebook, expandir la barra lateral desplegable de la dreta i al menú 'Input' clickar el botó 'Upload' i pujar el fitxer descarregat prèviament. Després cal donar un nom a la base de dades i quan es carregui el fitxer ja tindreu accessible la base de dades a la ruta <code> ../ input/</code>.\n\nUn cop tinguem la base de dades accessible, la inspeccionarem.\n\nA la carpeta <code>/images</code> (si treballem a casa) o <code>/kaggle/input/nom-base-de-dades/images</code> (si treballem des de Kaggle) trobem 2 carpetes:\n- A la carpeta <code>/train</code> es troba el total de les imatges d'entrenament separades per classes (cada classe en una carpeta diferent).\n- A la carpeta <code>/val</code>, es troba el total de les imatges de validació separades per classes (cada classe en una carpeta diferent).\n\nCom podem veure, tenim imatges per dur a terme l'entrenament i la validació dels models, però no tenim un conjunt de prova, el crearem durant aquesta primera secció.\n\nComencem per obtenir les dades i analitzar la seva estructura i característiques.\n### 1.1. Anàlisi dels fitxers d'imatges\n\nPrimer, inspeccionarem l’organització de les dades.","metadata":{"id":"1lyqUYZQjTlu"}},{"cell_type":"markdown","source":"<div style = \"background-color: #edf7ff; border-color: #7c9dbf; frontera-esquerra: 5px sòlid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 1.1 [0,5 pts]: </strong> A partir de l'estructura de carpetes indicada i per a cada conjunt de dades (<code>/train</code> i <code>/val</code>):\n    <ul>\n        <li> Extreu els noms de les 10 classes de cada conjunt i comproveu que siguin iguals.</li>\n        <li> Obteniu quantes instàncies hi ha en total per a cada classe i representa la distribució en un diagrama de barres.</li>\n        </ul>\nRespon també les preguntes següents:\n    <ul>\n        <li> Quin percentatge de les imatges totals corresponen a cada conjunt (train/val)?</li>\n        <li> Segons els diagrames de barres, podem dir que la distribució per classes és similar en els dos conjunts?</li>\n        <li> Pel que fa al nombre d’imatges per classe, les classes estan equilibrades o hi ha desequilibris notables?</li>\n    </ul>\n<strong> Nota: es valorarà la concisió en la resposta a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"tGZD0UyLjTlv"}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Directoris (ajustar la ruta segons ubicació de les dades)\ndataset_path = '/kaggle/input/m2-975-pac1-images/images'\n# List directories\nfor folder in sorted(os.listdir(dataset_path)):\n    path = os.path.join(dataset_path, folder)\n    if os.path.isdir(path):\n        print(f\"📂 {folder}\")\n        for subfolder in sorted (os.listdir(path)):\n            print(\"  \" + f\"📂 {subfolder}\")","metadata":{"id":"e8axDSGlmsdr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Llistar classes a cada conjunt\nval_classes = os.listdir('/kaggle/input/m2-975-pac1-images/images/val')\nprint(val_classes)\ntrain_classes = os.listdir('/kaggle/input/m2-975-pac1-images/images/train')\nprint(train_classes)","metadata":{"trusted":true,"id":"0w-IVqP9jTlv"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificar que les dues llistes de classes són iguals\n[os.listdir('/kaggle/input/m2-975-pac1-images/images/val')]==[os.listdir('/kaggle/input/m2-975-pac1-images/images/train')]","metadata":{"trusted":true,"id":"liIf5ENkjTlw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_path = Path('/kaggle/input/m2-975-pac1-images/images/val')\ntrain_path = Path('/kaggle/input/m2-975-pac1-images/images/train')\n\n# Imprimeix nombre d'imatges per classe a train\ncount_train = {}\nfor f in sorted (os.listdir(train_path)):\n    count_train[f]=len(os.listdir(os.path.join(train_path, f)))\nprint(count_train)\n\n# Imprimeix nombre d'imatges per classe a val\ncount_val = {}\nfor f in sorted (os.listdir(val_path)):\n    count_val[f]=len(os.listdir(os.path.join(val_path, f)))\nprint(count_val)\n\n# Comptar total d'imatges per classe\ncount_total = count_train.copy()\nfor k, v in count_val.items():\n        count_total[k]+=v\nprint(count_total)\n\n# Comptar total d'imatges per conjunt\ncount_train_total = sum(count_train.values())\ncount_val_total = sum(count_val.values())\ncount_total_values = sum(count_total.values())\nprint(f\" El nombre total d'imatges per a l'entrenament és de: {count_train_total}\")\nprint(f\" El nombre total d'imatges per al test és de: {count_val_total}\")\nprint(f\" El nombre total d'imatges és de: {count_total_values}\")\n","metadata":{"trusted":true,"id":"oe1Ymr-njTlw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Representació gràfica de la distribució de classes\nval=plt.bar(count_val.keys(), count_val.values(), label='Test', color='skyblue')\ntrain=plt.bar(count_train.keys(), count_train.values(), bottom=list(count_val.values()), label = 'Train', color='lightcoral')\nplt.xlabel('Class')\nplt.ylabel('Number of images')\nplt.title('Total images per class')\nplt.legend()\nplt.bar_label(val,label_type='center')\nplt.bar_label(train,label_type='center')\nplt.tick_params(axis='x', labelrotation=90)\nplt.show()\n\n# Referències\n# https://www.datacamp.com/es/tutorial/python-bar-plot\n# https://stackoverflow.com/questions/10998621/rotate-axis-tick-labels","metadata":{"id":"L1wGYqjJnKaW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n        <ul>\n        <li> <strong> Quin percentatge de les imatges totals correspon a cada conjunt (train/val)?</strong> 29,3% val i 70,7% train</li>\n        <li> <strong> Segons els diagrames de barres, podem afirmar que la distribució per classes és similar en els dos conjunts?</strong> És força similar en totes les classes.</li>\n        <li> <strong> Pel que fa al nombre d’imatges per classe, les classes estan equilibrades o hi ha desequilibris notables?</strong> No hi ha desequilibris notables</li>\n    </ul>     \n<br> <br>\n</div>","metadata":{"id":"kia0PMupjTlx"}},{"cell_type":"markdown","source":"### 1.2. Visualització d’imatges.\n\nAra examinarem el format de les imatges per entendre la seva mida i el seu rang de valors. Visualitzarem algunes imatges d’exemple de cada classe.","metadata":{"id":"j088_-85jTlx"}},{"cell_type":"markdown","source":"<div style = \"background-color: #edf7ff; border-color: #7c9dbf; frontera-esquerra: 5px sòlid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 1.2 [0,5 pts]: </strong> Mostra com a exemple una imatge per a cada categoria del conjunt d'entrenament. A continuació, respon:\n    <ul>\n        <li> Tenen totes les imatges les mateixes dimensions (alçada × amplada)?\n        <li> Quin és el rang dinàmic dels píxels (valors mínim i màxim) a les imatges?\n        <li> Aquests valors són adequats per entrenar una xarxa neuronal?\n        <li> Quines accions de pre-processament serien necessàries en relació a la pregunta anterior?\n    </ul>\n<strong> Nota: es valorarà la concisió en la resposta a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"P3ZpQtnbjTlx"}},{"cell_type":"code","source":"import random\nfrom PIL import Image\n\ntrain_path = '/kaggle/input/m2-975-pac1-images/images/train'\ncollage = Image.new(\"RGBA\", (1000,4000))\nimg_h = 0\nfor f in sorted (os.listdir(train_path)):\n    img = Image.open(os.path.join(train_path, f,random.choice(os.listdir(os.path.join(train_path, f)))))\n    collage.paste(img, (0,img_h))\n    img_h+=img.size[1]\ncollage\n\n# Referències\n# https://docs.python.org/3/library/random.html\n# https://pillow.readthedocs.io/en/stable/reference/Image.html","metadata":{"trusted":true,"id":"cbyG-g-KjTlx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import islice\nmin_width = 1000\nmin_height = 1000\nmin_rgb = [0,0,0]\nmax_rgb = [0,0,0]\nfor folder in islice(sorted (os.listdir(train_path)),0,3):\n    for file in islice(sorted (os.listdir(os.path.join(train_path, folder))),0,3):\n        img = Image.open(os.path.join(train_path, folder,file))\n        width, height = img.size\n        if ((width != min_width) or (height != min_height)):\n            min_width = min(min_width, width)\n            min_height = min(min_height, height)\n        pixels = list(img.getdata())\n        for pixel in pixels:\n                min_rgb = (min(min_rgb[0], pixel[0]), min(min_rgb[1], pixel[1]), min(min_rgb[2], pixel[2]))\n                max_rgb = (max(max_rgb[0], pixel[0]), max(max_rgb[1], pixel[1]), max(max_rgb[2], pixel[2]))\nprint(min_width, min_height, min_rgb, max_rgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n        <ul>\n        <li> <strong> Mida </strong>: Les imatges tenen una mida diferent. </li>\n        <li> <strong> Rang dinàmic </strong>: Per a cada canal els valors mínim i màxim són 0 i 255 respectivament. </li>\n        <li> <strong> Adequació per a la xarxa </strong>: No són adequades. </li>\n        <li> <strong> Accions necessàries </strong>: Per tal d'adequar les dades als requeriments de la xarxa convé uniformitzar la mida de les imatges i normalitzar els valors dels píxels. </li>\n    </ul>\n</div>","metadata":{"id":"oK0d9km7jTly"}},{"cell_type":"markdown","source":"### 1.3. Creació dels conjunts de dades en format Keras/TensorFlow\n\nA continuació, prepararem les dades per a l'entrenament amb Keras. Utilitzarem la funció <code>**tf.keras.utils.image_dataset_from_directory()**</code> de TensorFlow/Keras, que permet crear lots de dades etiquetats en funció dels directoris d'imatges organitzats per classe.\n\nLa documentació d'aquesta funció es troba tant al lloc web de [Keras](https://keras.io/api/data_loading/image/) com al de [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory).\n\nAquesta funció ens facilitarà generar conjunts d'entrenament, validació i prova a partir de les carpetes analitzades. Les imatges es re-dimensionaran a una mida fixa i s’organitzaran en lots (batch).\n\n**Especificacions**: convertirem les imatges a la mida de 160 × 160 píxels i les agruparem en lots de 64 imatges. També separarem part de les dades de prova a partir de les de validació.\n\n\n<div style = \"background-color: #edf7ff; border-color: #7c9dbf; fronter-esquerra: 5px sòlid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 1.3 [0,5 pts]: </strong> Utilitzeu la funció <code>tf.keras.utils.image_dataset_from_directory () </code> per generar 3 conjunts de dades a partir de les carpetes <code>/train</code> i <code>/val</code>:\n<ul>\n  <li><strong>train_ds</strong>: Conjunt d'entrenament a partir de <code>/train/</code>. Redimensiona les imatges a 160×160, amb <code>batch_size=64</code> i <code>label_mode=\"categorical\"</code> (10 categories).</li>\n  <li><strong>val_ds</strong>: Conjunt de validació a partir del directori <code>/val/</code>, amb un <code>validation_split</code> de 0.5 usant <code>subset=\"validation\"</code> a la funció i fixa un <code>seed</code> per a reproducibilitat. De nou redimensiona les imatges a 160x160 amb <code>batch_size=64</code> i <code>label_mode=\"categorical\"</code>.</li>\n  <li><strong>test_ds</strong>: Conjunt de test a partir del mateix directori <code>/val/</code> usant <code>subset=\"training\"</code> a la funció (amb el mateix <code>seed</code> per obtenir la partició complementària). Redimensiona també a 160×160 amb <code>batch_size=64</code> i fes servir <code>label_mode=\"categorical\"</code>.</li>\n </ul>\n<strong>NOTA</strong>: l'assignació de <code>subset=\"validation\"</code> i <code>subset=\"training\"</code> és completament arbitrària i podria haver-se fet al revés. L'important és dividir les dades que es troben a la carpeta <code>/val</code> al 50% entre validació i test.\n <br><br>\nComproveu que la base de dades s'ha creat correctament imprimint per pantalla els noms de les classes i les dimensions de cada conjunt de dades.\n\nRespon les preguntes següents:\n <ul>\n  <li>Quin percentatge aproximat de les imatges s'utilitza a cadascun dels 3 conjunts?</li>\n  <li>Quins altres percentatges se solen utilitzar?</li>\n </ul>\n<strong>NOTA: Es valorarà la concisió en la resposta a les preguntes (una o dues frases són suficients per cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"oKg2bex6jTly"}},{"cell_type":"code","source":"img_height, img_width = 160, 160\nbatch_size = 64\n\n# Conjunt d'entrenament\ntrain_ds = tf.keras.utils.image_dataset_from_directory('/kaggle/input/m2-975-pac1-images/images/train',image_size = (img_height, img_width),\nbatch_size = batch_size, label_mode='categorical')","metadata":{"trusted":true,"id":"gyEToMxFjTly"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conjunt de validació i test (usem el directori de validació d'Imagenette també per a test)\nval_ds, test_ds = tf.keras.utils.image_dataset_from_directory('/kaggle/input/m2-975-pac1-images/images/val',image_size = (img_height, img_width),\nbatch_size = batch_size, validation_split = 0.5, label_mode='categorical', subset = 'both', seed = 1)","metadata":{"trusted":true,"id":"q2Imdz-3jTly"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comprovació dels resultats\nprint(train_ds.class_names)\nprint(len(train_ds))\nprint(test_ds.class_names)\nprint(tf.data.experimental.cardinality(test_ds).numpy())\nprint(val_ds.class_names)\nprint(val_ds.cardinality().numpy())","metadata":{"id":"_9qV7ez3jTly","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<br> <br>\n        <ul>\n        <li> <strong>Percentatge d'imatges per conjunt</strong>: train_ds (~70%), test_ds (~15%), val_ds (~15%)</li>\n        <li> <strong>Altres percentatges</strong>: Es solen utilitzar percentatges de ~60-80% per entrenament, ~10-20% per validació i ~10-20% per test.</li>\n        </ul>\n<br>\n<strong>Referències:</strong>\n<br> <br>\n    <ul>\n        <li>https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory </li>\n        <li>https://www.tensorflow.org/api_docs/python/tf/data/Dataset</li>\n        <li>https://www.tensorflow.org/tutorials/load_data/images</li>\n        <li>https://www.tensorflow.org/tutorials/keras/classification</li>\n    </ul>\n</div>","metadata":{"id":"M4mPjiSkjTly"}},{"cell_type":"markdown","source":"## 2. Xarxa neuronal artificial (ANN) completament connectada (1,5 pts)\n\nCom a primer model, entrenarem una xarxa neuronal **completament connectada** (xarxa densa o <it>Multilayer Perceptron</it>) per **classificar les imatges**. Això servirà de model de referència (línia de base). Atès que la gestió d’imatges de 160×160 píxels directament amb capes denses donaria lloc a vectors d’entrada molt grans i, per tant, un gran nombre de paràmetres, primer aplicarem una reducció de la dimensionalitat de les imatges d’entrada. Concretament, reduirem cada imatge a 32×32 píxels (mitjançant una capa de redimensionat) abans d’aplanar-la, que simplificarà el model.\n\n**Arquitectura proposada**: Utilitzarem l’API funcional de Keras (classe Model) per construir la xarxa. Utilitzarem les capes Resizing i Rescaling de Keras per preparar les imatges, seguides de Flatten per aplanar-les, i diverses capes Dense intercalades amb Dropout pel classificador.\n\nEn aquesta secció utilitzarem les capes [Resizing](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/resizing/), [Rescaling](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/rescaling/), [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/), [Dense](https://keras.io/api/layers/core_layers/dense/) i [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) de Keras.\n\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 2.1 [0,5 pts]:</strong> Implementa un model <strong>funcional</strong> de Keras amb les següents característiques:\n<ul>\n   <li>Una capa que redueixi les dimensions d'entrada de (160,160) a (32,32).</li>\n   <li>Una capa de reescalat perquè els valors de píxel quedin entre 0 i 1.</li>\n   <li>Una capa Flatten per convertir la imatge reduïda en un vector 1D.</li>\n   <li>Una capa densa completament connectada que tingui un nombre de neurones equivalent a dos terços de la mida de la capa anterior, activació ReLU.</li>\n   <li>Una capa Dropout amb probabilitat 0.5.</li>\n   <li>Una altra capa densa de la meitat de la mida que la capa densa inmediatament anterior, activació ReLU.</li>\n   <li>Una altra capa Dropout amb probabilitat 0.5.</li>\n   <li>Una capa de sortida densa amb la mida i la funció d'activació adequades per al problema de classificació que es planteja.</li>\n </ul>\n<strong>Nota: Visualitzar els models de tots els exercicis amb la funció <code>summary()</code></strong>\n<br><br>\nContestar les preguntes següents:\n <ul>\n     <li>Quina mida hauria de tenir la capa d'entrada si haguéssim treballat amb les imatges de la base de dades directament de 160x160 píxels?</li>\n     <li>Quina reducció s'aconsegueix en reescalar les imatges a 32x32 píxels?</li>\n      <li>Quants paràmetres té el model?</li>\n </ul>\n\n<strong>NOTA: Es valorarà la concisió en la resposta a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"2ekbrupHjTly"}},{"cell_type":"code","source":"# Construcció del model ANN completament connectat (model funcional)\ndef get_uncompiled_model():\n    num_classes = 10\n    model = keras.Sequential([\n        keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n        keras.layers.Resizing(32, 32),\n        keras.layers.Rescaling(1./255),\n        keras.layers.Flatten(),\n        # La mida de la Flatten és 32 x 32 x 3 (RGB)\n        keras.layers.Dense(2048, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(1024, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(num_classes, activation='softmax')  # Sortida amb 10 classes\n    ])\n    return model\n    \nmodel = get_uncompiled_model()\nmodel.summary()\nkeras.utils.plot_model(model, \"model.png\", show_shapes=True,dpi=64)","metadata":{"trusted":true,"id":"dkjpG0P5jTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Càlcul del nombre de paràmetres\nsize_flatten_layer = 32*32*3+1\nprint(size_flatten_layer)\nparam_dense1_layer = size_flatten_layer*2048\nprint(param_dense1_layer)\nparam_dense2_layer = (2048+1)*1024\nprint(param_dense2_layer)\nparam_dense3_layer = (1024+1)*10\nprint(param_dense3_layer)\nparam_total = param_dense1_layer + param_dense2_layer + param_dense3_layer\nprint(param_total)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<br> <br>\n        <ul>\n        <li><strong>Quina mida hauria de tenir la capa d'entrada si haguéssim treballat amb les imatges de la base de dades directament de 160x160 píxels?</strong> Hauria de tenir una mida de 160x160x3 = 76800</li>\n     <li><strong>Quina reducció s'aconsegueix en reescalar les imatges a 32x32 píxels?</strong> S'aconsegueix una reducció del 96%</li>\n      <li><strong>Quants paràmetres té el model?</strong> Té 8.4 mil·lions de paràmetres.</li>\n        </ul>\n<br>\n    <strong>Referències:</strong>\n<br>\n    <li>https://www.tensorflow.org/tutorials/load_data/images</li>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_preprocessing/resizing/</li>\n    <li>https://keras.io/api/layers/core_layers/dense/</li>\n    <li>https://keras.io/api/layers/reshaping_layers/flatten/</li>\n    <li>https://keras.io/api/layers/regularization_layers/dropout/</li>\n</div>","metadata":{"id":"8ZWD_YHDjTlz"}},{"cell_type":"markdown","source":"Procedim a compilar i entrenar el model:\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 2.2 [1 PTS]: </strong> Quan entreneu qualsevol model, és essencial fer una bona elecció dels hiperparameters. En aquest exercici compilarem i entrenarem el model creat analitzant els efectes del <em>learning rate</em> en l'entrenament. Concretament, provarem els valors de 1e-3, 1e-4 i 1e-5.\n\nPer a cada valor de <em>learning rate</em> es demana:\n<ul>\n   <li> Compileu el model amb l'optimitzador <strong>Adam</strong> i el corresponent <em>learning rate</em>.</li>\n   <li>\n        Entrena el model durant <strong>100 epochs</strong>, mitjançant:\n        <ul>\n            <li><em>EarlyStopping</em> monitoritzant la pèrdua (<em>loss</em>) en validació, amb paciència de 10 epochs, restablint els millors pesos obtinguts.\n            <li><em>ReduceLROnPlateau</em> monitoritzant la pèrdua (<em>loss</em>) en validació, factor 0,2, paciència de 5 epochs i un <em>learning rate</em> mínim de 1e-6 </i>\n            <li> La mètrica de rendiment <em>accuracy</em> tant en entrenament com en validació.\n        </ul>\n   </li>\n   <li> Gràfic de de les corbes de <em>accuracy</em> i <em>loss</em> per als conjunts d'entrenament i validació.\n   <li> Finalment, avalueu el model entrenat al conjunt de test, mostrant la pèrdua (<em>loss</em>) i la precisió (<em>accuracy</em>) obtingudes en aquestes dades.\n</ul>\n\nFeu una comparació de l'efecte del <em>learning rate</em> en els 3 entrenaments basada en:\n    <ul>\n        <li> Número d’èpoques/temps d’entrenament.\n        <li> Forma de les corbes d’entrenament.\n        <li> Rendiment del model al conjunt de prova (test).\n        <li> Quina taxa d’aprenentatge seleccionaries en funció del que s'ha comentat en els punts anteriors?\n    </ul>\n\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"Q3biQwDL3zZ3"}},{"cell_type":"markdown","source":"Comencem amb lr = 1e-3","metadata":{"id":"AmZbaFNO3zZ4"}},{"cell_type":"code","source":"from timeit import default_timer as timer\n\n# Definició funció de compilació\ndef get_compiled_model(lr):\n    model = keras.models.clone_model(get_uncompiled_model())\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n    return model\n\n# Definició de callbacks\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, logs={}):\n        self.logs=[]\n    def on_epoch_begin(self, epoch, logs={}):\n        self.starttime = timer()\n    def on_epoch_end(self, epoch, logs={}):\n        self.logs.append(timer()-self.starttime)\n    def on_train_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(\"Stop training at epoch{}; got log keys: {}\",epoch,format(keys))\n\nearly_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_loss', patience=10, restore_best_weights=True)\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n\n# Definició funció per graficar accuracy i loss\ndef get_acc_loss_plot(lr,history):\n    plt.figure(figsize=(15, 5))\n    # Gràfic de loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label=f'Train (lr={lr})')\n    plt.plot(history.history['val_loss'], label=f'Val (lr={lr})')\n    \n    # Gràfic d'accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label=f'Train (lr={lr})')\n    plt.plot(history.history['val_accuracy'], label=f'Val (lr={lr})')\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Loss durant l'entrenament\")\n    plt.xlabel(\"Èpoques\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid()\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Accuracy durant l'entrenament\")\n    plt.xlabel(\"Èpoques\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid()\n    \n    plt.show()","metadata":{"trusted":true,"id":"x5AIVXhMjTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilació del model\nlr=1e-3\nmodel1 = get_compiled_model(lr)\nmodel1.summary()\nkeras.utils.plot_model(model1, \"model1.png\", show_shapes=True,dpi=64)","metadata":{"trusted":true,"id":"xx5HVAvLjTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament del model\ncustom_callback1 = CustomCallback()\n\nhistory1 = model1.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback1],\n    verbose=1\n)","metadata":{"trusted":true,"id":"x0nkaR-djTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\" El model {model1.name} s'ha entrenat durant {round(sum(custom_callback1.logs),2)} \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history1)\n\n# Validació al conjunt de prova\nresult1 = model1.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Repetim amb lr = 1e-4","metadata":{"id":"EfyQF7N83zZ5"}},{"cell_type":"code","source":"# Crear un nou model sense pesos previs\nlr=1e-4\nmodel2 = get_compiled_model(lr)\nprint(model2)\n\n# Entrenament del model\nhistory2 = model2.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true,"id":"qhPTemfD3zZ6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\" El model {model2.name} s'ha entrenat durant {round(sum(custom_callback.logs),2)} \")\n\n# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history2)\n\n# Validació al conjunt de prova\nresult2 = model2.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Acabem amb lr = 1e-5","metadata":{"id":"Vtxtdb1M3zZ6"}},{"cell_type":"code","source":"# Crear un nou model sense pesos previs\nlr=1e-5\nmodel3 = get_compiled_model(lr)\nprint(model3)\n\n# Entrenament del model\nhistory3 = model3.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true,"id":"mm3YFYpY3zZ6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history3)\n\n# Validació al conjunt de prova\nresult3 = model3.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n    <ul>\n        <li> <strong> Forma de les corbes d'entrenament </strong>: (escriu la teva resposta aquí)</li>\n        <li> <strong> Número d’èpoques/temps d’entrenament </strong>: (escriu la teva resposta aquí)</li>        \n        <li> <strong> Rendiment del model al conjunt de prova</strong>: (escriu la teva resposta aquí)</li>\n        <li> <strong>Elecció final </strong>: (escriu la teva resposta aquí)</li>\n    </ul>\n\n<strong>Referències</strong>:\n    <ul>\n        <li>https://keras.io/api/callbacks/</li>\n        <li>https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights</li>\n        <li>https://keras.io/guides/training_with_built_in_methods/</li>\n        <li>https://keras.io/api/models/model_training_apis/</li>\n        <li>https://keras.io/guides/writing_your_own_callbacks/</li>\n    </ul>\n</div>","metadata":{"id":"FccpVTAE3zZ7"}},{"cell_type":"markdown","source":"## 3. Petita xarxa convolucional (2 punts)\n\nAra implementarem una xarxa neuronal convolucional bàsica (CNN), que sol ser molt més eficaç per a la classificació de les imatges. Les CNN aprofiten l'estructura espacial de les dades mitjançant capes específiques que permeten extreure les característiques locals abans de la classificació.\n\n**Arquitectura proposada**: Utilitzarem un model de Keras Sequential per a aquesta CNN. Consistirà en un bloc d’extractor de característiques i, a continuació, un classificador dens similar a l'anterior però més petit.\n\nEn aquesta secció utilitzarem les capes  [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/),  [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/), [Dense](https://keras.io/api/layers/core_layers/dense/) i [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) de Keras.\n\nEs proporciona el codi del classificador ja implementat.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.1 [0,5 pts]:</strong> Analitza el codi següent i descriu les capes que formen el bloc extractor de característiques i el classificador.</div>","metadata":{"id":"Sh7d7lG_jTlz"}},{"cell_type":"code","source":"# Definició del model CNN\ncnn_model = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n], name=\"CNN_model\")\ncnn_model.summary()","metadata":{"trusted":true,"id":"GRjz2My8jTl0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Resposta: </strong>\n<br>\n <ul>\n     <li> <u>Bloc convolucional:</u> \n         <br>La primera capa normalitza els valors dels 3 canals a valors entre 0 i 1. \n         <br>La primera capa convolucional aplica 16 filtres de 3x3 en 3 canals amb s=1 i padding=1 (nombre de paràmetres = (3x3x3+1)x16 = 448), aplica la funció d'activació ReLU i redueix la dimensionalitat a la meitat amb un filtre max_pooling de 2x2. \n         <br>La segona capa convolucional aplica 32 filtres de 3x3 en 16 canals amb s=1 i padding=1 (nombre de paràmetres = (3x3x16+1)x32 = 4640), aplica la funció d'activació ReLU i redueix la dimensionalitat a la meitat amb un filtre max_pooling de 2x2.  \n         <br>La tercera capa convolucional aplica 64 filtres de 3x3 en 32 canals amb s=1 i padding=1 (nombre de paràmetres = (3x3x32+1)x64 = 18496), aplica la funció d'activació ReLU i redueix la dimensionalitat a la meitat amb un filtre max_pooling de 2x2. \n     </li>\n     <li> <u>Classificador final:</u>\n         <br>La primera capa FC inclou una capa dropout per tal de prevenir el sobreentrenament que desactiva un 20% de les entrades (només activa en la fase d'entrenament) i una capa densa de 64 neurones actuant sobre els 64 canals (nombre de paràmetres = (64+1)*64 = 4160).\n         <br>La segona capa FC inclou una capa dropout per tal de prevenir el sobreentrenament que desactiva un 50% de les entrades (només activa en la fase d'entrenament) i una capa densa de 10 neurones actuant sobre els 64 canals (nombre de paràmetres = (64+1)*10 = 650) activada amb 'softmax' per tal de donar-nos la sortida del classificador.\n     </li>\n <ul>\n</div>","metadata":{"id":"v8QJO2e5jTl0"}},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.2 [0.5 pts]</strong>: Compila el model amb l'optimitzador <strong>Adam</strong> i <em>learning rate</em> = 1e-4. Entrena el model durant <strong>100 èpoques</strong>, utilitzant:\n    <ul>\n        <li><em>EarlyStopping</em> monitoritzant la pèrdua (<em>loss</em>) en validació, amb paciència de 10 èpoques, restaurant els millors pesos obtinguts.</li>\n        <li><em>ReduceLROnPlateau</em> monitoritzant la pèrdua (<em>loss</em>) en validació, factor 0.2, paciència de 5 èpoques i un learning rate mínim de 1e-6</i>\n        <li>La mètrica de rendiment <em>accuracy</em> tant en entrenament com en validació.</li>\n    </ul>\n\nMostra una gràfica de les corbes d'accuracy i loss per als conjunts d'entrenament i validació.\n\nFinalment, avalua el model entrenat sobre el conjunt de <test>test</test>, mostrant la pèrdua i exactitud (<it>accuracy</it>) final obtingudes en aquestes dades.\n\n<strong>Nota: durant l'execució del codi es produirà un error.</strong>\n\nExplica a què és degut l'error en l'execució anterior i modifica el model per solucionar-lo.\n\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"CkP9wfgF3zaG"}},{"cell_type":"code","source":"# Definició funció compilació\ndef get_compiled_model_cnn(lr,ref_model):\n    model = keras.models.clone_model(ref_model)\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"id":"GlgOsDS9jTl0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilar model CNN\nlr=1e-4\nmodel4 = get_compiled_model_cnn(lr,cnn_model)\nmodel4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Entrenament del model CNN\n# history4 = model4.fit(\n#     train_ds,\n#     validation_data=val_ds,\n#     epochs=100,\n#     batch_size=batch_size,\n#     callbacks=[early_stopping,reduce_lr,custom_callback],\n#     verbose=1\n# )\n\n# # Grafics d'accuracy i loss\n# get_acc_loss_plot(lr,history4)\n\n# # Validació al conjunt de prova\n# result4 = model4.evaluate(test_ds)","metadata":{"trusted":true,"id":"DGrNTcJijTl0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<br>\n<br>\nL'error es deu a què les dimensions output i target no són iguals ja que no s'ha aplanat la sortida.\n    <br> <br>\n<strong>Referències</strong>:\n    <ul>\n        <li>https://keras.io/api/layers/convolution_layers/convolution2d/</li>\n        <li>https://keras.io/api/layers/pooling_layers/max_pooling2d/</li>\n        <li>https://keras.io/api/layers/regularization_layers/dropout/</li>\n        <li>https://keras.io/guides/serialization_and_saving/</li>\n    </ul>\n</div>","metadata":{"id":"dKiBjmms3zaH"}},{"cell_type":"code","source":"# Escriu aquí el model corregit\ncnn_model_c = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Flatten(),\n    Dense(10, activation='softmax')\n])\ncnn_model_c.summary()","metadata":{"trusted":true,"id":"MSeN6pUQ3zaH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.3 [0.5 pt]:</strong> Torna a repetir l'exercici anterior (entrenament, mostrar gràfiques i validació en el conjunt de test) amb el model corregit. Comenta la mida del model, el nombre d'epochs/temps d'entrenament, les corbes d'entrenament i els resultats de validació.\n<br><br>\n<strong>NOTA: Es valorarà la concisió en la resposta a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).</strong>\n\n</div>","metadata":{"id":"TZN8ZypU3zaI"}},{"cell_type":"code","source":"# Compilar model CNN\nlr=1e-4\nmodel5 = get_compiled_model_cnn(lr,cnn_model_c)\nmodel5","metadata":{"trusted":true,"id":"ktByKLKB3zaI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament del model CNN\nhistory5 = model5.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr],\n    verbose=1\n)","metadata":{"trusted":true,"id":"IdcQLyLs3zaJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history5)\n\n# Validació al conjunt de prova\nresult5 = model5.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Escriu aquí el model corregit\ncnn_model_d = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\ncnn_model_d.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilar model CNN\nlr=1e-4\nmodel6 = get_compiled_model_cnn(lr,cnn_model_d)\n\n# Entrenament del model CNN\nhistory6 = model6.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history6)\n\n# Validació al conjunt de prova\nresult6 = model6.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>cnn_model_c:</strong>\n<br>El model té una mida de: 283.754 paràmetres.\n<br>El nombre d'epochs d'entrenament ha estat de xxx amb un temps de xxx, les corbes d'entrenament i els resultats de validació.\n<br> <br>\n<strong>cnn_model_d:</strong>\n<br>El model té una mida de: 1.662.698 paràmetres.\n<br>El nombre d'epochs d'entrenament ha estat de xxx amb un temps de xxx, les corbes d'entrenament i els resultats de validació.\n<br> <br>\n</div>","metadata":{"id":"-wT_GLeL3zaJ"}},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.4 [0.5 pt]:</strong> Realitza una comparativa entre els models dels exercicis 2 (amb el <em>learning rate</em> escollit) i 3 en termes de:\n<ul>\n  <li> <strong> Mida del model </strong> </li>\n  <li> <strong> Evolució de l'entrenament </strong> </li>\n  <li> <strong> Validació en el conjunt de prova </strong> </li>\n  <li> <strong> Temps d'execució </strong> </li>\n</ul>\n\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).\n\n</div>","metadata":{"id":"9zqrLKeD3zaK"}},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<ul>\n  <li> <strong>Mida del model</strong>: (escriu la teva resposta aquí)</li>\n  <li> <strong>Evolució de l'entrenament</strong>: (escriu la teva resposta aquí)</li>\n  <li> <strong>Validació en el conjunt de prova</strong>: (escriu la teva resposta aquí)</li>\n  <li> <strong>Temps d'execució</strong>: (escriu la teva resposta aquí)</li>\n</ul>\n</div>","metadata":{"id":"jpeJpPyj3zaL"}},{"cell_type":"markdown","source":"## 4. Augmentació de dades (1 punt)\n\nTot i que la CNN entrenada és força efectiva, podem intentar millorar-ne la generalització augmentant artificialment la mida i la diversitat del conjunt d'entrenament mitjançant **tècniques d'augmentació de dades**. L'augmentació consisteix a aplicar transformacions aleatòries a les imatges (girs, rotacions, zoom, etc.) de manera que el model rebi variants de les imatges originals a cada època, simulant tenir més dades\n\nKeras proporciona capes de preprocessament d'imatge que realitzen aquestes transformacions de manera eficient durant l'entrenament, per exemple RandomFlip, RandomRotation, RandomZoom entre d'altres.\n\nAquí farem servir algunes d'aquestes capes per implementar l'augment. En particular, provarem de voltejar horitzontalment les imatges i aplicar petites rotacions aleatòries.\n\n### 4.1. Definició i visualització de l'augmentació\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 4.1 [0,5 pts]:</strong> Crea un model <strong>Sequential</strong> de Keras que representi una capa d'augmentació de dades amb les següents capes:\n  <ul>\n      <li><code>RandomFlip</code> que voltegi aleatòriament les imatges horitzontalment.</li>\n      <li><code>RandomRotation</code> amb factor de rotació de 0.1 (±10%).</li>\n  </ul>\nSelecciona una imatge del conjunt d'entrenament i passa-la a través d'aquest model diverses vegades, mostrant les imatges resultants per comprovar visualment les transformacions realitzades (voltejos i rotacions).\n</div>","metadata":{"id":"4YFqSk28jTl0"}},{"cell_type":"code","source":"# Model d'augmentació de dades\nmodel_aug = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    RandomFlip(mode='horizontal'),\n    RandomRotation(0.1)\n])","metadata":{"trusted":true,"id":"ziTG9gAdjTl4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prendre un batch d'entrenament i obtenir una imatge\nrd_class_path = os.path.join(train_path,random.choice(os.listdir(train_path)))\nrd_image_path = os.path.join(rd_class_path,random.choice(os.listdir(rd_class_path)))\n\n# Càrrega de la imatge i conversió a tensor\nimg = keras.preprocessing.image.load_img(rd_image_path, target_size= (img_height, img_width))\nimg_tensor = keras.preprocessing.image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\n\n# Normalitzem els valors al rang [0,1]\nimg_tensor /= 255.\n\n# Grafiquem la imatge\nplt.figure(figsize=(6,6))\nplt.imshow(img_tensor[0])\nplt.title('Imatge Original')\nplt.axis('off')\nplt.show()\n\n# Aplicar augmentació diverses vegades i visualitzar\nplt.figure(figsize=(12, 12))\nfor i in range(9):\n    augmented_img = model_aug(img_tensor, training=True)\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_img[0].numpy())\n    plt.axis(\"off\")\nplt.suptitle('Imatges Augmentades')\nplt.show()","metadata":{"trusted":true,"id":"FWJoHXhujTl5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2. Entrenament de la CNN amb augmentació\n\nAra incorporarem la capa d’augmentació al model CNN per entrenar-la amb dades augmentades a cada època.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 4.2 [0,5 pts]:</strong> Insereix el model d'augmentació de dades creat entre la capa de <code>Rescaling</code> i la primera <code>Conv2D</code> del model CNN anterior. És a dir, modifica l'arquitectura perquè les imatges d'entrada, després de ser reescalades, passin per les capes de <em>flip</em> i <em>rotation</em> aleatòries, i després continuïn per la CNN. A continuació, compila i entrena el nou model CNN augmentat seguint les mateixes indicacions que en l'exercici anterior, <strong>excepte</strong> que aquesta vegada farem servir un <strong>learning rate</strong> inicial més gran (1e-3). Mantingues EarlyStopping, ReduceLROnPlateau, 100 èpoques, etc. Després avalua el model final al conjunt de test.\n\nComenta les diferències amb el model sense augmentació en termes de:\n\n<ul>\n    <li> <strong> Evolució de les corbes d'entrenament </strong> </li>\n    <li> <strong> Validació en el conjunt de prova </strong> </li>\n    <li> <strong> Temps d'entrenament </strong> </li>\n    <li> <strong> Learning rate </strong> </li>\n</ul>\n\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).\n\n</div>","metadata":{"id":"m732VSK1jTl5"}},{"cell_type":"code","source":"# Model CNN amb augmentació de dades\ncnn_model_aug = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    RandomFlip(mode='horizontal'),\n    RandomRotation(0.1),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\ncnn_model_aug.summary()","metadata":{"trusted":true,"id":"RQIGxy4WjTl5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilació de la xarxa\nlr=1e-3\nmodel7 = get_compiled_model_cnn(lr,cnn_model_aug)","metadata":{"trusted":true,"id":"iqRmCU2DjTl5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament del model CNN\nhistory7 = model7.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true,"id":"sKXAr1DUjTl5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resultats\n# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history6)\n\n# Validació al conjunt de prova\nresult7 = model7.evaluate(test_ds)","metadata":{"trusted":true,"id":"VP3Q9ArDjTl6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n\n<ul>\n  <li> <strong> Evolució de les corbes d'entrenament</strong>: (escriu la teva resposta aquí) </li>\n  <li> <strong> Validació en el conjunt de prova</strong>: (escriu la teva resposta aquí) </li>\n  <li> <strong> Temps d'entrenament</strong>: (escriu la teva resposta aquí) </li>\n  <li> <strong> Learning rate</strong>: (escriu la teva resposta aquí) </li>\n</ul>\n<br>\n<strong> Referències: </strong>\n<br> <br>\n\n<ul>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/</li>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_augmentation/</li>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/</li>\n    <li>https://gac6.medium.com/visualizing-data-augmentations-from-keras-image-data-generator-44f040aa4c9f</li>\n<li>chatgpt.com</li>\n</ul>\n\n</div>","metadata":{"id":"3GX8JzB9jTl6"}},{"cell_type":"markdown","source":"## 5. Superresolució d’imatges amb CNNs (2 pts)\n\nFins ara hem treballat en un problema de “classificació”. Als apartats restants abordarem un problema diferent però relacionat amb la visió per computador: la **superresolució d'imatges**. La superresolució consisteix a generar una imatge d'alta resolució (HR) a partir d'una de baixa resolució (LR), intentant recuperar o inferir els detalls perduts en reduir la imatge. És un problema d'aprenentatge supervisat on el model aprèn una transformació imatge->imatge.\n\nFarem servir novament la base de dades Imagenette per crear exemples d'entrenament: a partir de cada imatge original (320px) generarem una versió reduïda (p. ex. 80px) que servirà com a entrada, tenint com a sortida esperada la imatge original. Així, el model aprendrà a mapejar de baixa a alta resolució. En lloc d'una xarxa convolucional per a la classificació, necessitarem una xarxa capaç de processar una imatge d'entrada i produir una imatge de sortida. Les **capes transposades de convolució** ([Conv2DTranspose](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/)) o les tècniques de upsampling són les peces clau per a aquests models, ja que permeten augmentar les dimensions espacials de les dades.\n\nA continuació, crearem el conjunt de dades per a superresolució i entrenarem una **CNN de superresolució simple**.\n\n### 5.1. Preparació de dades de LR/HR\n\nPrimer generarem els parells d'imatges d'entrenament i de validació per a superresolució. Partirem de les imatges originals d'entrenament (i validació) a la seva resolució completa i, per simplicitat, les redimensionarem a una mida fixa de 320×320 (ignorant la relació d'aspecte original, similar al fet en classificació) per utilitzar-les com a imatges de referència d'alta resolució (HR), i les reduirem a 1/4 de la seva mida (aprox). Per realitzar ambdues transformacions (ajust de les imatges originals a 320x320 i la seva reducció a 80x80) utilitzarem mètodes d'[interpolació bicúbica](https://es.wikipedia.org/wiki/Interpolaci%C3%B3n_bic%C3%BAbica) per simular imatges degrades suaument.\n\n\nProcedim a obtenir les rutes d'imatges d'entrenament i validació, després creem un Dataset aplicant la funció de mapeig que realitza la lectura i la transformació:\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 5.1.1 [0,5 pt]: </strong>El codi següent prepara un nou conjunt de dades a partir de les imatges d'entrenament originals. Cada exemple consistirà en una imatge de baixa resolució (LR) i la corresponent imatge d'alta resolució (HR). Concretament, pren les imatges de <code>/train/</code> (per crear els subconjunts d'entrenament i validació) a la seva mida completa (320px) com a HR, i genera imatges LR reduint-les a 1/4 de la seva mida lineal (80×80). Es demana:\n<ul>\n    <li> Completa els comentaris del codi en aquells llocs on està marcat (# Posar Comentari).\n</ul>\n</div>","metadata":{"id":"SIPuOf5bjTl6"}},{"cell_type":"code","source":"# Funció per al preprocessat d'imatges per al problema de SR\ndef preprocess_image_for_sr(filepath):\n    # Càrrega i descodificació del fitxer\n    img = tf.io.read_file(filepath)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Conversió a tensor float32 (valors entre 0 i 1)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # Reescalat mitjançant interpolació bicúbica\n    hr = tf.image.resize(img, [320, 320], method=tf.image.ResizeMethod.BICUBIC)\n    lr = tf.image.resize(img, [80, 80], method=tf.image.ResizeMethod.BICUBIC)\n    # Normalització dels valors del tensor\n    hr = tf.clip_by_value(hr, 0.0, 1.0)\n    lr = tf.clip_by_value(lr, 0.0, 1.0)\n    return lr, hr\n\n# Obtenim els conjunts d'entrenament i validació\ntrain_files = []\nval_files = []\nfor cls in train_classes:\n    cls_files = random.shuffle((train_path/cls).glob(\"*.*\"))\n    # Separem els conjunts d'entrenament (80%) i validació (20%)\n    split_idx = int(len(cls_files) * 0.2)\n    val_files += [str(p) for p in cls_files[:split_idx]]\n    train_files += [str(p) for p in cls_files[split_idx:]]\nrandom.shuffle(train_files)\nrandom.shuffle(val_files)\n\n# Creem els datasets d'entrenament i validació\ntrain_sr_ds = tf.data.Dataset.from_tensor_slices(train_files) # Rutes\n.map(preprocess_image_for_sr, num_parallel_calls=tf.data.AUTOTUNE) # LR, HR\n.batch(32) # Agrupació lots de 32\n.prefetch(tf.data.AUTOTUNE) # Optimització processament\nval_sr_ds   = tf.data.Dataset.from_tensor_slices(val_files)\n.map(preprocess_image_for_sr, num_parallel_calls=tf.data.AUTOTUNE)\n.batch(32)\n.prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"id":"MxjJviEfjTl7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Procedim a verificar la base de dades:","metadata":{"id":"1udwGw3K3zaR"}},{"cell_type":"code","source":"# Verificació d'un parell LR-HR\nfor lr_batch, hr_batch in train_sr_ds.take(1):\n    lr_img = lr_batch[0].numpy()\n    hr_img = hr_batch[0].numpy()\n    print(\"LR shape:\", lr_img.shape, \"HR shape:\", hr_img.shape)\n    print(\"LR pixel range:\", lr_img.min(), \"-\", lr_img.max())\n    print(\"HR pixel range:\", hr_img.min(), \"-\", hr_img.max())\n    break\nprint(f\"El nombre d'imatges en el conjunt d'entrenament és {len(train_files)}\")\nprint(f\"El nombre d'imatges en el conjunt de validació és {len(val_files)}\")","metadata":{"trusted":true,"id":"VRFA2ufvjTl7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style = \"background-color: #edf7ff; border-color: #7c9dbf; frontera-esquerra: 5px sòlid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 5.1.2 [0,5 pt]: </strong> Respon a les preguntes següents:\n<ul>\n <li>A què es pot deure l'ús de la funció 'clip_by_value' al codi anterior?</li>\n <li>Com es divideix la base de dades? Quin directori s'utilitza per crear-la? Quins subconjunts es creen? Quin percentatge d'imatges s'assigna a cada conjunt? I en nombre d'imatges?</li>\n <li>Per a cada element del conjunt de dades, quin format tenen les dades que farem servir per entrenar el model? i les etiquetes?</li>\n</ul>\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"DcNsu5Zd3zaS"}},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n <ul>\n    <li> <strong> Funció 'clip_by_value' </strong>: La funció ens permet corregir possibles valors fora del rang [0,1] generats en la interpolació bicúbica. </li>\n    <li> <strong> Subconjunts de la base de dades </strong>: La base de dades es divideix en un conjunt d'entrenament que conté un 80% de les dades (7578 imatges) i un conjunt de validació que conté el 20% restant (1891 imatges). </li>\n    <li> <strong> Format de les dades i etiquetes </strong>: Les dades d'entrada (LR) tenen una mida de 80x80 píxels representats en un tensor (80,80,3). Les etiquetes (HR) tenen una mida de 320x320 píxels representats en un tensor (320,320,3) </li>\n</ul>\n<strong> Referències: </strong>\n<br> <br>\n <ul>\n     <li>https://www.tensorflow.org/api_docs/python/tf/clip_by_value</li>\n     <li>chatgpt.com</li>\n</ul>\n</div>","metadata":{"id":"gkOrAEFsjTl8"}},{"cell_type":"markdown","source":"### 5.2. Implementació del model de Superresolució\n\nAra definim un model CNN per a Superresolució. Optarem per una arquitectura senzilla amb [UpSampling2D](https://keras.io/api/layers/reshaping_layers/up_sampling2d/) per escalar la imatge gradualment. Implementem el model:\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 5.2 [1 pt]: </strong> Entrenarem un model CNN per superresolució 4× (de 80px a 320px). Per fer això:\n    <ul>\n        <li> Implementa un model de superresolució a Keras. Podeu utilitzar l’API funcional o Sequential. Suggeriment d'arquitectura simple:\n            <ul>\n                <li> Entrada amb forma (80,80,3).\n                <li> Diverses capes Conv2D per extreure les característiques de les imatges de baixa resolució.\n                <li> Capa de <code>Upsampling2D</code> (factor=2) o <code>Conv2DTranspose</code> amb <code>stride=2</code> per doblar l'amplada i alçada (80 -> 160).\n                <li> Una altra capa Conv2D per refinar, seguida d'una altra capa upsampling 2× (160 -> 320).\n                <li> Capes finals Conv2D per reconstruir la imatge de sortida 320×320×3. (Podeu utilitzar l'activació lineal o RELU per a la sortida; considereu que busquem valors de píxel entre 0-1 o 0-255).\n            </ul> </li>\n        <li> Compileu el model amb una funció de pèrdua adequada per a imatges (per exemple, MSE) i sense mètriques de precisió. Entreneu-lo durant ~ 50-100 epochs, amb EarlyStopping (paciència 5-10, monitorització de val_loss) per assegurar-vos que no sobreentreneu.\n        <li> Comentar els resultats de l'entrenament. Concretament:\n            <ul>\n                <li> Corbes d'entrenament.\n                <li> El temps d'entrenament vs Tamany del model.\n            </ul>                \n        </li>\n    </ul>\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"S-PYhtzP3zaT"}},{"cell_type":"code","source":"# Model de Superrsolució (80px -> 320px)\n# Model de Superresolució (80px -> 320px)\ninputs = keras.Input(shape=(80, 80, 3))\n\n# Extracció de característiques\nx = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\nx = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n\n# Primera fase d'upsampling (80 -> 160)\nx = layers.UpSampling2D(size=2)(x)\nx = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n\n# Segona fase d'upsampling (160 -> 320)\nx = layers.UpSampling2D(size=2)(x)\nx = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n\n# Refinament i reconstrucció\nx = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\noutputs = layers.Conv2D(3, 3, padding='same', activation='sigmoid')(x)  # Sortida entre 0-1","metadata":{"id":"8Ifu1K1Z7gVi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilació de la xarxa\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer='adam', loss='mse')\n\n# Resum del model\nmodel.summary()","metadata":{"trusted":true,"id":"s7qM9vifjTl7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament de la xarxa\n","metadata":{"trusted":true,"id":"-qEbXQxEjTl7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resultats\n","metadata":{"trusted":true,"id":"4d4o0XQ4jTl7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n<ul>\n    <li> <strong> Corbes d’entrenament </strong>: (escriviu la vostra resposta aquí) </li>\n    <li> <strong> Temps d'entrenament vs Tamany del model </strong>: (escriviu la vostra resposta aquí) </li>\n</ul>\n</div>","metadata":{"id":"9Kmeo87v3zaW"}},{"cell_type":"markdown","source":"## 6. Inferència, mètriques i visualització dels resultats (1 punt)\n\nAmb el model de superresolució entrenat, avaluarem el seu rendiment tant quantitativament (amb mètriques) com qualitativament (visualització d’imatges). Per fer -ho, realitzarem la inferència: és a dir, prendre imatges LR del conjunt de test, passar-les pel model <code>sr_model</code> per obtenir imatges supers resoltes (SR) i comparar-les amb les imatges HR originals. Calcularem la mètrica [PSNR (Peak Signal-to-Notise Ratio)](https://es.wikipedia.org/wiki/psnr) per quantificar la qualitat.\n\nEl PSNR es mesura en decibels (dB) i els valors superiors indiquen una major similitud amb la imatge original (per exemple,> 30 dB sol indicar una qualitat de reconstrucció molt bona).\n\nTambé visualitzarem alguns exemples, on mostraren la imatge de baixa resolució (LR), la super resolta per la CNN (SR) i l’original d’alta resolució (HR), per inspeccionar els detalls amb l’ull nu.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 6 [1 pt]:</strong> Realitza la inferència del model de superresolució sobre diverses imatges de test:\n<ul>\n  <li>Per a cada imatge de test, obté la seva versió de baixa resolució (LR) i genera la imatge superresolta (SR) amb el model.</li>\n  <li>Calcula la mètrica PSNR comparant la imatge SR amb la HR original per a tot el conjunt de test.</li>\n  <li>Mostra visualment alguns exemples, incloent-hi la imatge d'entrada LR (pots ampliar-la per mostrar-la de la mateixa mida utilitzant el mètode <code>tf.image.resize()</code> juntament amb <code>method=tf.image.ResizeMethod.NEAREST_NEIGHBOR</code>), la imatge generada pel model (SR) i la imatge HR original, per poder comparar la qualitat visualment a part del valor de la PSNR.</li>\n</ul>\nComentar els resultats obtinguts:\n<ul>\n    <li> Quins valors de PSNR s’obtenen? (Comenta tant els exemples mostrats com el valor mitjà del total del conjunt de prova) </li>\n    <li> Les imatges creades amb superresolució són nítides o borroses?\n    <li> Quins tipus de detalls o errors s’observen?\n</ul>\n<strong> Nota: es valorarà la concisió a les preguntes (una o dues frases són suficients per a cadascuna de les respostes).  \n</div>","metadata":{"id":"piSdBJmP8f81"}},{"cell_type":"code","source":"# Preparar conjunt de test per a SR (a partir del directori /val )\n","metadata":{"trusted":true,"id":"4hqRsPE_3zaY"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Llista per emmagatzemar la PSNR de cada imatge\n\n# Recórrer el dataset de test (batch size = 1)\n\n # Generar la imatge de superresolució a partir de la imatge LR\n\n # Treure la dimensió de batch i assegurar que els valors estiguin a [0,1]\n\n # Convertir la imatge HR a array de numpy (llevem la dimensió de batch)\n\n # Calcular la PSNR entre la imatge SR generada i la imatge HR original\n\n # Emmagatzemar el valor de PSNR a la llista\n\n\n# Calcular la PSNR mitjana del conjunt de test\navg_psnr =\nprint(\"PSNR mitjana al conjunt de test: {:.2f} dB\".format(avg_psnr))","metadata":{"trusted":true,"id":"WR3mXWV33zaZ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\n# Escollir algunes imatges de test aleatòries per a visualització\n\n# Per a cada imatge\n\n # Llegir i processar una imatge de test\n\n # Generar superresolució\n\n # Calcular PSNR\n\n # Preparar imatges per visualitzar de la mateixa mida\n\n # Les imatges estan a [0,1], escalar a [0,255] per mostrar correctament\n\n # Mostrar les imatges\n\n","metadata":{"trusted":true,"id":"9gpUyeINjTl-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n   Anàlisi dels resultats del model SR:\n   <ul>\n       <li> <strong> PSNR </strong>: (escriviu la vostra resposta aquí) </li>\n       <li> <strong> Qualitat visual </strong>: (escriviu la vostra resposta aquí) </li>\n       <li> <strong> Errors </strong>: (escriviu la vostra resposta aquí) </li>\n   </ul>\n        \n   (Escriviu la vostra resposta aquí)\n</div>","metadata":{"id":"UVHiHLuSjTl-"}},{"cell_type":"markdown","source":"## 7. Model pre-entrenat en superresolució (1 punt)\n\nEls avenços recents en superresolució han produït arquitectures més complexes (p. ex., basades en xarxes generatives adversàries) que aconsegueixen resultats notablement millors, a costa d'un entrenament costós. Un d'aquests models és ESRGAN ([Enhanced Super-Resolution GAN per Xintao Wang et al.](https://arxiv.org/abs/1809.00219)), entrenat en grans bases de dades d'imatges HD (com DIV2K) per aconseguir superresolució 4x de gran fidelitat.\n\nFarem servir un model pre-entrenat d'ESRGAN disponible via [TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/image_enhancing)\n\nAquest model ha estat entrenat al conjunt DIV2K (imatges d'alta qualitat) amb degradació bicúbica, per la qual cosa està especialitzat en produir imatges 4× més grans amb detall notable.\n\nFarem servir ESRGAN per aplicar superresolució a les mateixes imatges de test i compararem els resultats amb el nostre model implementat a l'exercici 5.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 7 [1 pt]:</strong> Utilitza un model pre-entrenat de superresolució (p. ex. ESRGAN 4×) per millorar les imatges de baixa resolució. En concret:\n  <ul>\n      <li>Carrega el model pre-entrenat des de TensorFlow Hub (usa l'URL proporcionada, per exemple <code>\"https://tfhub.dev/captain-pool/esrgan-tf2/1\"</code>).</li>\n      <li>Per a una selecció d'imatges de test de baixa resolució (80×80), obteniu la superresolució 4× amb aquest model.</li>\n      <li>Calcula la PSNR de les imatges generades pel model pre-entrenat (en el total del conjunt de test) i compara la seva mitjana amb la PSNR mitjana obtinguda pel teu model.</li>\n      <li>Visualitza les imatges de sortida del model pre-entrenat al costat de les del teu model i les originals, per comparar visualment la qualitat. Compara també la PSNR d'aquestes imatges.</li>\n  </ul>\nDiscuteix les diferències observades:\n  <ul>\n      <li>El model pre-entrenat obté millors mètriques?</li>\n      <li>El model pre-entrenat produeix imatges més nítides i properes a la realitat?</li>\n      <li>En quins detalls es nota la millora?</li>\n  </ul>\n</div>","metadata":{"id":"tJHyY2JjjTl_"}},{"cell_type":"code","source":"import tensorflow_hub as hub\n\n# Carreguem el model ESRGAN des de TF-Hub:\nesrgan = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\nprint(\"Model ESRGAN carregat.\")","metadata":{"trusted":true,"id":"9SiuIY5WjTl_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Llista per emmagatzemar la PSNR de cada imatge usant ESRGAN\n\n# Recórrer el dataset de test (batch size = 1)\n\n # Preparar la imatge LR per ESRGAN:\n # ESRGAN espera entrada en rang [0,255] com a float32, i test_sr_ds té imatges a [0,1]\n\n # Generar la imatge de superresolució amb ESRGAN\n\n # Treure la dimensió de batch i normalitzar la sortida a [0,1]\n\n # Obtenir la imatge HR corresponent (llevem la dimensió de batch)\n\n # Calcular la PSNR entre la imatge generada per ESRGAN i la imatge HR original\n\n\n# Calcular la PSNR mitjana en el conjunt de test amb ESRGAN\navg_psnr_esrgan =\nprint(\"PSNR mitjana en el conjunt de test amb ESRGAN: {:.2f} dB\".format(avg_psnr_esrgan))","metadata":{"trusted":true,"id":"HWdLPlYU3zaf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apliquem ESRGAN a les mateixes imatges d'exemple que fem servir amb el nostre model\n\n# Per a cada imatge\n\n # Preparar la imatge LR de test\n\n # Convertir sortida a [0,1] float\n\n # Calcular PSNR comparat amb HR\n\n # Visualitzar comparativa\n","metadata":{"id":"hyWzmyZc-Umu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<ul>\n    <li> <strong> Mètrica (PSNR) </strong>: (escriviu la vostra resposta aquí) </li>\n    <li> <strong> Qualitat visual </strong>: (escriviu la vostra resposta aquí) </li>\n    <li> <strong> Detalls </strong>: (escriviu la vostra resposta aquí) </li>\n</ul>  \n</div>","metadata":{"id":"Jn2JeQrUjTmA"}},{"cell_type":"markdown","source":"## 8. Comparació i conclusions (0,5 pts)\n\nEn aquesta pràctica hem explorat tant la classificació d’imatges amb xarxes neuronals (denses vs convolucionals) com la superresolució amb CNNs, aplicant-ho tot sobre la base de dades d’Imagenette.","metadata":{"id":"nX8PEeKp-znz"}},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 8 [0,5 pts]: </strong> Fes un resum dels punts apresos en aquest PAC. Concretament, resumeix en una frase el que has après de cadascun dels punts enumerats a continuació: <ul>\n    <li> <strong> ANN vs CNN: </strong> </li>\n    <li> <strong> Regularització i augmentació: </strong> </li>\n    <li> <strong> Superresolució: </strong> </li>\n    <li> <strong> Transfer learning: </strong> </li>\n</ul>\nValoració final\n</div>","metadata":{"id":"TtxSISOB3zag"}},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\nConclusions finals:\n<ul>\n    <li>\n        <strong> ANN vs CNN: </strong> (escriviu la vostra resposta aquí)\n    </li>\n    <li>\n        <strong> Regularització i augmentació: </strong> (escriviu la vostra resposta aquí)\n    </li>\n    <li>\n        <strong> Superresolució: </strong> (escriviu la vostra resposta aquí)\n    </li>\n    <li>\n        <strong> Transfer Learning: </strong> (escriviu la vostra resposta aquí)\n    </li>\n</ul>\n\n\n  (Escriviu la vostra resposta aquí)\n<br> <br>\n</div>","metadata":{"id":"X_W9yTgs-0NA"}}]}