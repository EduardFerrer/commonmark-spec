{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10893751,"sourceType":"datasetVersion","datasetId":6770004},{"sourceId":11115228,"sourceType":"datasetVersion","datasetId":6930425}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width: 100%; clear: both;\">\n<div style=\"float: left; width: 50%;\">\n<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n</div>\n<div style=\"float: right; width: 50%;\">\n<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.975 路 Deep Learning 路 PAC1</p>\n<p style=\"margin: 0; text-align:right;\">2024-2 路 Mster universitari en Ci猫ncia de dades (Data science)</p>\n<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis de Informtica, Multim猫dia y Telecomunicaci贸</p>\n</div>\n</div>\n<div style=\"width:100%;\">&nbsp;</div>\n\n# PEC 1: Xarxes neuronals artificials i convolucionals amb Keras - Classificaci贸 dimatges i superresoluci贸\n\nAl llarg d'aquesta prctica, implementarem diversos models de xarxes neuronals, utilitzant Keras i la base de dades Imagenette (versi贸 320px). Concretament, abordarem les tasques seg眉ents:\n1. Descrrega, anlisi i pre-processament de les dades (1,5 pts)\n2. Xarxa neuronal artificial (ANN) completament connectada (1,5 pts)\n3. Petita xarxa neuronal convolucional (CNN) (2 pts)\n4. Augmentaci贸 de dades (1 pt)\n5. Superresoluci贸 dimatges amb CNNs (1,5 pts)\n6. Infer猫ncia, m猫triques i visualitzaci贸 dels resultats (1 pt)\n7. Model Pre-entrenat en Superresoluci贸 (1 pt)\n8. Comparaci贸 i conclusions (0,5 pts)\n\n<u> Consideracions generals </u>:\n\n- La soluci贸 proposada no pot utilitzar m猫todes, funcions o parmetres declarats **_deprecated_** en versions futures.\n- Aquest PAC sha de fer duna manera **estrictament individual**. Qualsevol indici de c貌pia ser penalitzat amb un suspens (D) per a totes les parts implicades i la possible avaluaci贸 negativa de la totalitat de l'assignatura.\n- Cal que l'estudiant indiqui **totes les fonts** que ha utilitzat per a la realitzaci贸 del PEC. En cas contrari, es considerar que lalumne ha com猫s el plagi, sent penalitzat amb un suspens (D) i la possible avaluaci贸 negativa de la totalitat de l'assignatura.\n- Si sutilitza qualsevol **IA generativa** en la resoluci贸 de la PAC **sha de referenciar** en aquelles seccions on sha utilitzat, com qualsevol altra font.\n\n<u>Format del lliurament </u>:\n\n- Alguns exercicis poden suposar diversos minuts dexecuci贸, de manera que el lliurament sha de fer en format **Notebook** i en el format **html**, on es vegi el codi, els resultats i els comentaris de cada exercici. Podeu exportar el quadern a HTML a Jupyter Notebook des del men煤 File $\\to$ Download as $\\to$ HTML.\n- Hi ha un tipus especial de cel路la per allotjar el text. Aquest tipus de cel路les ser molt 煤til per respondre a les diferents preguntes te貌riques plantejades al llarg de lactivitat. Per canviar el tipus de cel路la a aquest tipus, al men煤: Cell $\\to$ Cell Type $\\to$ Markdown.","metadata":{"id":"QiqrZa4FjTln"}},{"cell_type":"markdown","source":"## 0. Context i crrega de llibreries\nAl llarg daquesta prctica, implementarem diversos models de xarxes neuronals per classificar les imatges de la base de dades [Imagenette](https://github.com/fastai/imagenette).\n\nLa base de dades Imagenette 茅s un subconjunt de 10 classes fcilment classificables de [Imagenet](https://www.image-net.org/), un projecte fonamental per avan莽ar en la investigaci贸 sobre visi贸 artificial i aprenentatge profund. Imagenette cont茅 unes 13.000 imatges de diferents mides pertanyents a 10 categories (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute), cadascuna en una carpeta diferent.\n\nConcretament en aquesta PAC, utilitzarem una versi贸 (Imagenette2-320) que ha estat re-escalada, per貌 mantenint la relaci贸 daspecte de cada imatge (shan ajustat de manera que la dimensi贸 menor de cada imatge sigui de 320 p铆xels). Aix貌 atenuar la crrega computacional dels algoritmes quan sutilitzen bases de dades dimatges, per貌 mantenint una qualitat suficient necessria per als nostres experiments. Les dades venen separades en 2 conjunts, entrenament i validaci贸.\n\n**Nota: A causa de l煤s dimatges com dades daquesta prctica, lentrenament de cada exercici es pot retardar entre uns minuts i m茅s de mitja hora mitjan莽ant GPU (els temps utilitzant CPU s贸n significativament m茅s llargs). Es recomana realitzar la prctica en lentorn que ofereix la plataforma Kaggle, ja que ofereix un entorn gratu茂t amb 30 hores setmanals per a l煤s de la GPU.**\n\nAl llarg de tota la prctica, per a la creaci贸 de les diferents xarxes, alternem l煤s del model [Sequential](https://keras.io/guides/sequential_model/) i el model [Functional](https://keras.io/guides/functional_api/) de Keras a trav茅s de les seves classes [Sequential](https://keras.io/api/models/sequential/) i [Model](https://keras.io/api/models/model/) respectivament.\n\nEs recomana la lectura detallada de la documentaci贸 dels dos models per dur a terme la realitzaci贸 de la prctica.\n\nComencem per instal路lar i carregar les llibreries m茅s rellevants:","metadata":{"id":"xZW_y0x-jTlr"}},{"cell_type":"code","source":"# Instal路lem la darrera versi贸 de Tensorflow (amb CUDA)\n%pip install tensorflow[and-cuda]","metadata":{"trusted":true,"id":"fpC54c2djTls","execution":{"iopub.status.busy":"2025-03-25T10:19:20.691306Z","iopub.execute_input":"2025-03-25T10:19:20.691604Z","iopub.status.idle":"2025-03-25T10:20:34.238084Z","shell.execute_reply.started":"2025-03-25T10:19:20.691583Z","shell.execute_reply":"2025-03-25T10:20:34.236957Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.37.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.26.4)\nCollecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow[and-cuda]) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow[and-cuda]) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow[and-cuda]) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow[and-cuda]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow[and-cuda]) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow[and-cuda]) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow[and-cuda]) (0.1.2)\nDownloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvcc-cu12\n    Found existing installation: nvidia-cuda-nvcc-cu12 12.6.85\n    Uninstalling nvidia-cuda-nvcc-cu12-12.6.85:\n      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.6.85\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Importem Tensorflow\nimport tensorflow as tf\nprint(\"TensorFlow version   : \", tf.__version__)\n\n# Necessitarem GPU\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n\n# Keras versi贸 is 3.5.0\nfrom tensorflow import keras\nprint(\"Keras version   : \", keras.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"r8y4L-MWjTlt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importem els elements de Keras que utilitzarem amb m茅s freq眉猫ncia\nfrom keras.utils import image_dataset_from_directory\nfrom keras.layers import (Input, Dense, Dropout, Flatten, Conv2D, Conv2DTranspose,\n                          MaxPooling2D, UpSampling2D, Rescaling, Resizing,\n                          RandomFlip, RandomRotation)\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.optimizers import Adam","metadata":{"trusted":true,"id":"2L3kwGpbjTlu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importem algunes llibreries que necessitarem per a la PAC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pathlib import Path","metadata":{"trusted":true,"id":"8rfYpHJhjTlu"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Descrrega, anlisi i dades pre-processades (1,5 punts)\n\nEn aquesta secci贸 explorarem la base de dades i prepararem la crrega de les imatges per als models de les seccions seg眉ents.\n\nPer crear la nostra base de dades, hem de descarregar el fitxer dimatges del seg眉ent [enlla莽](https://drive.google.com/file/d/16kqYxjXK0xDDYQ4xF3PBgsa0NG3gwT4L/view?usp=sharing) (茅s un fitxer .zip que ocupa aproximadament 340 Mb).\n\n**Nota: per descarregar el fitxer d'imatges, heu de iniciar la sessi贸 amb l'usuari i la contrasenya de la UOC.**\n\nA partir daqu铆:\n\n- Si treballem en local, simplement hem de descomprimir el fitxer descarregat.\n\n- Si treballem des de Kaggle hem de pujar el notebook de l'enunciat a la plataforma (per aix貌 podeu seguir els 6 primers passos del seg眉ent [article](https://rajputankit22.medium.com/how-to-upload-my-own-notebook-to-kaggle-2b0dedbb5a6b)) i despr茅s, un cop pujat el notebook, expandir la barra lateral desplegable de la dreta i al men煤 'Input' clickar el bot贸 'Upload' i pujar el fitxer descarregat pr猫viament. Despr茅s cal donar un nom a la base de dades i quan es carregui el fitxer ja tindreu accessible la base de dades a la ruta <code> ../ input/</code>.\n\nUn cop tinguem la base de dades accessible, la inspeccionarem.\n\nA la carpeta <code>/images</code> (si treballem a casa) o <code>/kaggle/input/nom-base-de-dades/images</code> (si treballem des de Kaggle) trobem 2 carpetes:\n- A la carpeta <code>/train</code> es troba el total de les imatges d'entrenament separades per classes (cada classe en una carpeta diferent).\n- A la carpeta <code>/val</code>, es troba el total de les imatges de validaci贸 separades per classes (cada classe en una carpeta diferent).\n\nCom podem veure, tenim imatges per dur a terme l'entrenament i la validaci贸 dels models, per貌 no tenim un conjunt de prova, el crearem durant aquesta primera secci贸.\n\nComencem per obtenir les dades i analitzar la seva estructura i caracter铆stiques.\n### 1.1. Anlisi dels fitxers d'imatges\n\nPrimer, inspeccionarem lorganitzaci贸 de les dades.","metadata":{"id":"1lyqUYZQjTlu"}},{"cell_type":"markdown","source":"<div style = \"background-color: #edf7ff; border-color: #7c9dbf; frontera-esquerra: 5px s貌lid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 1.1 [0,5 pts]: </strong> A partir de l'estructura de carpetes indicada i per a cada conjunt de dades (<code>/train</code> i <code>/val</code>):\n    <ul>\n        <li> Extreu els noms de les 10 classes de cada conjunt i comproveu que siguin iguals.</li>\n        <li> Obteniu quantes instncies hi ha en total per a cada classe i representa la distribuci贸 en un diagrama de barres.</li>\n        </ul>\nRespon tamb茅 les preguntes seg眉ents:\n    <ul>\n        <li> Quin percentatge de les imatges totals corresponen a cada conjunt (train/val)?</li>\n        <li> Segons els diagrames de barres, podem dir que la distribuci贸 per classes 茅s similar en els dos conjunts?</li>\n        <li> Pel que fa al nombre dimatges per classe, les classes estan equilibrades o hi ha desequilibris notables?</li>\n    </ul>\n<strong> Nota: es valorar la concisi贸 en la resposta a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"tGZD0UyLjTlv"}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Directoris (ajustar la ruta segons ubicaci贸 de les dades)\ndataset_path = '/kaggle/input/m2-975-pac1-images/images'\n# List directories\nfor folder in sorted(os.listdir(dataset_path)):\n    path = os.path.join(dataset_path, folder)\n    if os.path.isdir(path):\n        print(f\" {folder}\")\n        for subfolder in sorted (os.listdir(path)):\n            print(\"  \" + f\" {subfolder}\")","metadata":{"id":"e8axDSGlmsdr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Llistar classes a cada conjunt\nval_classes = os.listdir('/kaggle/input/m2-975-pac1-images/images/val')\nprint(val_classes)\ntrain_classes = os.listdir('/kaggle/input/m2-975-pac1-images/images/train')\nprint(train_classes)","metadata":{"trusted":true,"id":"0w-IVqP9jTlv"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificar que les dues llistes de classes s贸n iguals\n[os.listdir('/kaggle/input/m2-975-pac1-images/images/val')]==[os.listdir('/kaggle/input/m2-975-pac1-images/images/train')]","metadata":{"trusted":true,"id":"liIf5ENkjTlw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_path = Path('/kaggle/input/m2-975-pac1-images/images/val')\ntrain_path = Path('/kaggle/input/m2-975-pac1-images/images/train')\n\n# Imprimeix nombre d'imatges per classe a train\ncount_train = {}\nfor f in sorted (os.listdir(train_path)):\n    count_train[f]=len(os.listdir(os.path.join(train_path, f)))\nprint(count_train)\n\n# Imprimeix nombre d'imatges per classe a val\ncount_val = {}\nfor f in sorted (os.listdir(val_path)):\n    count_val[f]=len(os.listdir(os.path.join(val_path, f)))\nprint(count_val)\n\n# Comptar total d'imatges per classe\ncount_total = count_train.copy()\nfor k, v in count_val.items():\n        count_total[k]+=v\nprint(count_total)\n\n# Comptar total d'imatges per conjunt\ncount_train_total = sum(count_train.values())\ncount_val_total = sum(count_val.values())\ncount_total_values = sum(count_total.values())\nprint(f\" El nombre total d'imatges per a l'entrenament 茅s de: {count_train_total}\")\nprint(f\" El nombre total d'imatges per al test 茅s de: {count_val_total}\")\nprint(f\" El nombre total d'imatges 茅s de: {count_total_values}\")\n","metadata":{"trusted":true,"id":"oe1Ymr-njTlw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Representaci贸 grfica de la distribuci贸 de classes\nval=plt.bar(count_val.keys(), count_val.values(), label='Test', color='skyblue')\ntrain=plt.bar(count_train.keys(), count_train.values(), bottom=list(count_val.values()), label = 'Train', color='lightcoral')\nplt.xlabel('Class')\nplt.ylabel('Number of images')\nplt.title('Total images per class')\nplt.legend()\nplt.bar_label(val,label_type='center')\nplt.bar_label(train,label_type='center')\nplt.tick_params(axis='x', labelrotation=90)\nplt.show()\n\n# Refer猫ncies\n# https://www.datacamp.com/es/tutorial/python-bar-plot\n# https://stackoverflow.com/questions/10998621/rotate-axis-tick-labels","metadata":{"id":"L1wGYqjJnKaW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n        <ul>\n        <li> <strong> Quin percentatge de les imatges totals correspon a cada conjunt (train/val)?</strong> 29,3% val i 70,7% train</li>\n        <li> <strong> Segons els diagrames de barres, podem afirmar que la distribuci贸 per classes 茅s similar en els dos conjunts?</strong> s for莽a similar en totes les classes.</li>\n        <li> <strong> Pel que fa al nombre dimatges per classe, les classes estan equilibrades o hi ha desequilibris notables?</strong> No hi ha desequilibris notables</li>\n    </ul>     \n<br> <br>\n</div>","metadata":{"id":"kia0PMupjTlx"}},{"cell_type":"markdown","source":"### 1.2. Visualitzaci贸 dimatges.\n\nAra examinarem el format de les imatges per entendre la seva mida i el seu rang de valors. Visualitzarem algunes imatges dexemple de cada classe.","metadata":{"id":"j088_-85jTlx"}},{"cell_type":"markdown","source":"<div style = \"background-color: #edf7ff; border-color: #7c9dbf; frontera-esquerra: 5px s貌lid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 1.2 [0,5 pts]: </strong> Mostra com a exemple una imatge per a cada categoria del conjunt d'entrenament. A continuaci贸, respon:\n    <ul>\n        <li> Tenen totes les imatges les mateixes dimensions (al莽ada  amplada)?\n        <li> Quin 茅s el rang dinmic dels p铆xels (valors m铆nim i mxim) a les imatges?\n        <li> Aquests valors s贸n adequats per entrenar una xarxa neuronal?\n        <li> Quines accions de pre-processament serien necessries en relaci贸 a la pregunta anterior?\n    </ul>\n<strong> Nota: es valorar la concisi贸 en la resposta a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"P3ZpQtnbjTlx"}},{"cell_type":"code","source":"import random\nfrom PIL import Image\n\ntrain_path = '/kaggle/input/m2-975-pac1-images/images/train'\ncollage = Image.new(\"RGBA\", (1000,4000))\nimg_h = 0\nfor f in sorted (os.listdir(train_path)):\n    img = Image.open(os.path.join(train_path, f,random.choice(os.listdir(os.path.join(train_path, f)))))\n    collage.paste(img, (0,img_h))\n    img_h+=img.size[1]\ncollage\n\n# Refer猫ncies\n# https://docs.python.org/3/library/random.html\n# https://pillow.readthedocs.io/en/stable/reference/Image.html","metadata":{"trusted":true,"id":"cbyG-g-KjTlx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import islice\nmin_width = 1000\nmin_height = 1000\nmin_rgb = [0,0,0]\nmax_rgb = [0,0,0]\nfor folder in islice(sorted (os.listdir(train_path)),0,3):\n    for file in islice(sorted (os.listdir(os.path.join(train_path, folder))),0,3):\n        img = Image.open(os.path.join(train_path, folder,file))\n        width, height = img.size\n        if ((width != min_width) or (height != min_height)):\n            min_width = min(min_width, width)\n            min_height = min(min_height, height)\n        pixels = list(img.getdata())\n        for pixel in pixels:\n                min_rgb = (min(min_rgb[0], pixel[0]), min(min_rgb[1], pixel[1]), min(min_rgb[2], pixel[2]))\n                max_rgb = (max(max_rgb[0], pixel[0]), max(max_rgb[1], pixel[1]), max(max_rgb[2], pixel[2]))\nprint(min_width, min_height, min_rgb, max_rgb)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n        <ul>\n        <li> <strong> Mida </strong>: Les imatges tenen una mida diferent. </li>\n        <li> <strong> Rang dinmic </strong>: Per a cada canal els valors m铆nim i mxim s贸n 0 i 255 respectivament. </li>\n        <li> <strong> Adequaci贸 per a la xarxa </strong>: No s贸n adequades. </li>\n        <li> <strong> Accions necessries </strong>: Per tal d'adequar les dades als requeriments de la xarxa conv茅 uniformitzar la mida de les imatges i normalitzar els valors dels p铆xels. </li>\n    </ul>\n</div>","metadata":{"id":"oK0d9km7jTly"}},{"cell_type":"markdown","source":"### 1.3. Creaci贸 dels conjunts de dades en format Keras/TensorFlow\n\nA continuaci贸, prepararem les dades per a l'entrenament amb Keras. Utilitzarem la funci贸 <code>**tf.keras.utils.image_dataset_from_directory()**</code> de TensorFlow/Keras, que permet crear lots de dades etiquetats en funci贸 dels directoris d'imatges organitzats per classe.\n\nLa documentaci贸 d'aquesta funci贸 es troba tant al lloc web de [Keras](https://keras.io/api/data_loading/image/) com al de [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory).\n\nAquesta funci贸 ens facilitar generar conjunts d'entrenament, validaci贸 i prova a partir de les carpetes analitzades. Les imatges es re-dimensionaran a una mida fixa i sorganitzaran en lots (batch).\n\n**Especificacions**: convertirem les imatges a la mida de 160  160 p铆xels i les agruparem en lots de 64 imatges. Tamb茅 separarem part de les dades de prova a partir de les de validaci贸.\n\n\n<div style = \"background-color: #edf7ff; border-color: #7c9dbf; fronter-esquerra: 5px s貌lid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 1.3 [0,5 pts]: </strong> Utilitzeu la funci贸 <code>tf.keras.utils.image_dataset_from_directory () </code> per generar 3 conjunts de dades a partir de les carpetes <code>/train</code> i <code>/val</code>:\n<ul>\n  <li><strong>train_ds</strong>: Conjunt d'entrenament a partir de <code>/train/</code>. Redimensiona les imatges a 160160, amb <code>batch_size=64</code> i <code>label_mode=\"categorical\"</code> (10 categories).</li>\n  <li><strong>val_ds</strong>: Conjunt de validaci贸 a partir del directori <code>/val/</code>, amb un <code>validation_split</code> de 0.5 usant <code>subset=\"validation\"</code> a la funci贸 i fixa un <code>seed</code> per a reproducibilitat. De nou redimensiona les imatges a 160x160 amb <code>batch_size=64</code> i <code>label_mode=\"categorical\"</code>.</li>\n  <li><strong>test_ds</strong>: Conjunt de test a partir del mateix directori <code>/val/</code> usant <code>subset=\"training\"</code> a la funci贸 (amb el mateix <code>seed</code> per obtenir la partici贸 complementria). Redimensiona tamb茅 a 160160 amb <code>batch_size=64</code> i fes servir <code>label_mode=\"categorical\"</code>.</li>\n </ul>\n<strong>NOTA</strong>: l'assignaci贸 de <code>subset=\"validation\"</code> i <code>subset=\"training\"</code> 茅s completament arbitrria i podria haver-se fet al rev茅s. L'important 茅s dividir les dades que es troben a la carpeta <code>/val</code> al 50% entre validaci贸 i test.\n <br><br>\nComproveu que la base de dades s'ha creat correctament imprimint per pantalla els noms de les classes i les dimensions de cada conjunt de dades.\n\nRespon les preguntes seg眉ents:\n <ul>\n  <li>Quin percentatge aproximat de les imatges s'utilitza a cadascun dels 3 conjunts?</li>\n  <li>Quins altres percentatges se solen utilitzar?</li>\n </ul>\n<strong>NOTA: Es valorar la concisi贸 en la resposta a les preguntes (una o dues frases s贸n suficients per cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"oKg2bex6jTly"}},{"cell_type":"code","source":"img_height, img_width = 160, 160\nbatch_size = 64\n\n# Conjunt d'entrenament\ntrain_ds = tf.keras.utils.image_dataset_from_directory('/kaggle/input/m2-975-pac1-images/images/train',image_size = (img_height, img_width),\nbatch_size = batch_size, label_mode='categorical')","metadata":{"trusted":true,"id":"gyEToMxFjTly"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conjunt de validaci贸 i test (usem el directori de validaci贸 d'Imagenette tamb茅 per a test)\nval_ds, test_ds = tf.keras.utils.image_dataset_from_directory('/kaggle/input/m2-975-pac1-images/images/val',image_size = (img_height, img_width),\nbatch_size = batch_size, validation_split = 0.5, label_mode='categorical', subset = 'both', seed = 1)","metadata":{"trusted":true,"id":"q2Imdz-3jTly"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comprovaci贸 dels resultats\nprint(train_ds.class_names)\nprint(len(train_ds))\nprint(test_ds.class_names)\nprint(tf.data.experimental.cardinality(test_ds).numpy())\nprint(val_ds.class_names)\nprint(val_ds.cardinality().numpy())","metadata":{"id":"_9qV7ez3jTly","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<br> <br>\n        <ul>\n        <li> <strong>Percentatge d'imatges per conjunt</strong>: train_ds (~70%), test_ds (~15%), val_ds (~15%)</li>\n        <li> <strong>Altres percentatges</strong>: Es solen utilitzar percentatges de ~60-80% per entrenament, ~10-20% per validaci贸 i ~10-20% per test.</li>\n        </ul>\n<br>\n<strong>Refer猫ncies:</strong>\n<br> <br>\n    <ul>\n        <li>https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory </li>\n        <li>https://www.tensorflow.org/api_docs/python/tf/data/Dataset</li>\n        <li>https://www.tensorflow.org/tutorials/load_data/images</li>\n        <li>https://www.tensorflow.org/tutorials/keras/classification</li>\n    </ul>\n</div>","metadata":{"id":"M4mPjiSkjTly"}},{"cell_type":"markdown","source":"## 2. Xarxa neuronal artificial (ANN) completament connectada (1,5 pts)\n\nCom a primer model, entrenarem una xarxa neuronal **completament connectada** (xarxa densa o <it>Multilayer Perceptron</it>) per **classificar les imatges**. Aix貌 servir de model de refer猫ncia (l铆nia de base). At猫s que la gesti贸 dimatges de 160160 p铆xels directament amb capes denses donaria lloc a vectors dentrada molt grans i, per tant, un gran nombre de parmetres, primer aplicarem una reducci贸 de la dimensionalitat de les imatges dentrada. Concretament, reduirem cada imatge a 3232 p铆xels (mitjan莽ant una capa de redimensionat) abans daplanar-la, que simplificar el model.\n\n**Arquitectura proposada**: Utilitzarem lAPI funcional de Keras (classe Model) per construir la xarxa. Utilitzarem les capes Resizing i Rescaling de Keras per preparar les imatges, seguides de Flatten per aplanar-les, i diverses capes Dense intercalades amb Dropout pel classificador.\n\nEn aquesta secci贸 utilitzarem les capes [Resizing](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/resizing/), [Rescaling](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/rescaling/), [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/), [Dense](https://keras.io/api/layers/core_layers/dense/) i [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) de Keras.\n\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 2.1 [0,5 pts]:</strong> Implementa un model <strong>funcional</strong> de Keras amb les seg眉ents caracter铆stiques:\n<ul>\n   <li>Una capa que redueixi les dimensions d'entrada de (160,160) a (32,32).</li>\n   <li>Una capa de reescalat perqu猫 els valors de p铆xel quedin entre 0 i 1.</li>\n   <li>Una capa Flatten per convertir la imatge redu茂da en un vector 1D.</li>\n   <li>Una capa densa completament connectada que tingui un nombre de neurones equivalent a dos ter莽os de la mida de la capa anterior, activaci贸 ReLU.</li>\n   <li>Una capa Dropout amb probabilitat 0.5.</li>\n   <li>Una altra capa densa de la meitat de la mida que la capa densa inmediatament anterior, activaci贸 ReLU.</li>\n   <li>Una altra capa Dropout amb probabilitat 0.5.</li>\n   <li>Una capa de sortida densa amb la mida i la funci贸 d'activaci贸 adequades per al problema de classificaci贸 que es planteja.</li>\n </ul>\n<strong>Nota: Visualitzar els models de tots els exercicis amb la funci贸 <code>summary()</code></strong>\n<br><br>\nContestar les preguntes seg眉ents:\n <ul>\n     <li>Quina mida hauria de tenir la capa d'entrada si hagu茅ssim treballat amb les imatges de la base de dades directament de 160x160 p铆xels?</li>\n     <li>Quina reducci贸 s'aconsegueix en reescalar les imatges a 32x32 p铆xels?</li>\n      <li>Quants parmetres t茅 el model?</li>\n </ul>\n\n<strong>NOTA: Es valorar la concisi贸 en la resposta a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).</strong>\n</div>","metadata":{"id":"2ekbrupHjTly"}},{"cell_type":"code","source":"# Construcci贸 del model ANN completament connectat (model funcional)\ndef get_uncompiled_model():\n    num_classes = 10\n    model = keras.Sequential([\n        keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n        keras.layers.Resizing(32, 32),\n        keras.layers.Rescaling(1./255),\n        keras.layers.Flatten(),\n        # La mida de la Flatten 茅s 32 x 32 x 3 (RGB)\n        keras.layers.Dense(2048, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(1024, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(num_classes, activation='softmax')  # Sortida amb 10 classes\n    ])\n    return model\n    \nmodel = get_uncompiled_model()\nmodel.summary()\nkeras.utils.plot_model(model, \"model.png\", show_shapes=True,dpi=64)","metadata":{"trusted":true,"id":"dkjpG0P5jTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clcul del nombre de parmetres\nsize_flatten_layer = 32*32*3+1\nprint(size_flatten_layer)\nparam_dense1_layer = size_flatten_layer*2048\nprint(param_dense1_layer)\nparam_dense2_layer = (2048+1)*1024\nprint(param_dense2_layer)\nparam_dense3_layer = (1024+1)*10\nprint(param_dense3_layer)\nparam_total = param_dense1_layer + param_dense2_layer + param_dense3_layer\nprint(param_total)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<br> <br>\n        <ul>\n        <li><strong>Quina mida hauria de tenir la capa d'entrada si hagu茅ssim treballat amb les imatges de la base de dades directament de 160x160 p铆xels?</strong> Hauria de tenir una mida de 160x160x3 = 76800</li>\n     <li><strong>Quina reducci贸 s'aconsegueix en reescalar les imatges a 32x32 p铆xels?</strong> S'aconsegueix una reducci贸 del 96%</li>\n      <li><strong>Quants parmetres t茅 el model?</strong> T茅 8.4 mil路lions de parmetres.</li>\n        </ul>\n<br>\n    <strong>Refer猫ncies:</strong>\n<br>\n    <li>https://www.tensorflow.org/tutorials/load_data/images</li>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_preprocessing/resizing/</li>\n    <li>https://keras.io/api/layers/core_layers/dense/</li>\n    <li>https://keras.io/api/layers/reshaping_layers/flatten/</li>\n    <li>https://keras.io/api/layers/regularization_layers/dropout/</li>\n</div>","metadata":{"id":"8ZWD_YHDjTlz"}},{"cell_type":"markdown","source":"Procedim a compilar i entrenar el model:\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 2.2 [1 PTS]: </strong> Quan entreneu qualsevol model, 茅s essencial fer una bona elecci贸 dels hiperparameters. En aquest exercici compilarem i entrenarem el model creat analitzant els efectes del <em>learning rate</em> en l'entrenament. Concretament, provarem els valors de 1e-3, 1e-4 i 1e-5.\n\nPer a cada valor de <em>learning rate</em> es demana:\n<ul>\n   <li> Compileu el model amb l'optimitzador <strong>Adam</strong> i el corresponent <em>learning rate</em>.</li>\n   <li>\n        Entrena el model durant <strong>100 epochs</strong>, mitjan莽ant:\n        <ul>\n            <li><em>EarlyStopping</em> monitoritzant la p猫rdua (<em>loss</em>) en validaci贸, amb paci猫ncia de 10 epochs, restablint els millors pesos obtinguts.\n            <li><em>ReduceLROnPlateau</em> monitoritzant la p猫rdua (<em>loss</em>) en validaci贸, factor 0,2, paci猫ncia de 5 epochs i un <em>learning rate</em> m铆nim de 1e-6 </i>\n            <li> La m猫trica de rendiment <em>accuracy</em> tant en entrenament com en validaci贸.\n        </ul>\n   </li>\n   <li> Grfic de de les corbes de <em>accuracy</em> i <em>loss</em> per als conjunts d'entrenament i validaci贸.\n   <li> Finalment, avalueu el model entrenat al conjunt de test, mostrant la p猫rdua (<em>loss</em>) i la precisi贸 (<em>accuracy</em>) obtingudes en aquestes dades.\n</ul>\n\nFeu una comparaci贸 de l'efecte del <em>learning rate</em> en els 3 entrenaments basada en:\n    <ul>\n        <li> N煤mero d猫poques/temps dentrenament.\n        <li> Forma de les corbes dentrenament.\n        <li> Rendiment del model al conjunt de prova (test).\n        <li> Quina taxa daprenentatge seleccionaries en funci贸 del que s'ha comentat en els punts anteriors?\n    </ul>\n\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"Q3biQwDL3zZ3"}},{"cell_type":"markdown","source":"Comencem amb lr = 1e-3","metadata":{"id":"AmZbaFNO3zZ4"}},{"cell_type":"code","source":"from timeit import default_timer as timer\n\n# Definici贸 funci贸 de compilaci贸\ndef get_compiled_model(lr):\n    model = keras.models.clone_model(get_uncompiled_model())\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n    return model\n\n# Definici贸 de callbacks\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, logs={}):\n        self.logs=[]\n    def on_epoch_begin(self, epoch, logs={}):\n        self.starttime = timer()\n    def on_epoch_end(self, epoch, logs={}):\n        self.logs.append(timer()-self.starttime)\n    def on_train_end(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(\"Stop training at epoch{}; got log keys: {}\",epoch,format(keys))\n\nearly_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_loss', patience=10, restore_best_weights=True)\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n\n# Definici贸 funci贸 per graficar accuracy i loss\ndef get_acc_loss_plot(lr,history):\n    plt.figure(figsize=(15, 5))\n    # Grfic de loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label=f'Train (lr={lr})')\n    plt.plot(history.history['val_loss'], label=f'Val (lr={lr})')\n    \n    # Grfic d'accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label=f'Train (lr={lr})')\n    plt.plot(history.history['val_accuracy'], label=f'Val (lr={lr})')\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Loss durant l'entrenament\")\n    plt.xlabel(\"poques\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid()\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Accuracy durant l'entrenament\")\n    plt.xlabel(\"poques\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid()\n    \n    plt.show()","metadata":{"trusted":true,"id":"x5AIVXhMjTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilaci贸 del model\nlr=1e-3\nmodel1 = get_compiled_model(lr)\nmodel1.summary()\nkeras.utils.plot_model(model1, \"model1.png\", show_shapes=True,dpi=64)","metadata":{"trusted":true,"id":"xx5HVAvLjTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament del model\ncustom_callback1 = CustomCallback()\n\nhistory1 = model1.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback1],\n    verbose=1\n)","metadata":{"trusted":true,"id":"x0nkaR-djTlz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\" El model {model1.name} s'ha entrenat durant {round(sum(custom_callback1.logs),2)} \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history1)\n\n# Validaci贸 al conjunt de prova\nresult1 = model1.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Repetim amb lr = 1e-4","metadata":{"id":"EfyQF7N83zZ5"}},{"cell_type":"code","source":"# Crear un nou model sense pesos previs\nlr=1e-4\nmodel2 = get_compiled_model(lr)\nprint(model2)\n\n# Entrenament del model\nhistory2 = model2.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true,"id":"qhPTemfD3zZ6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\" El model {model2.name} s'ha entrenat durant {round(sum(custom_callback.logs),2)} \")\n\n# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history2)\n\n# Validaci贸 al conjunt de prova\nresult2 = model2.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Acabem amb lr = 1e-5","metadata":{"id":"Vtxtdb1M3zZ6"}},{"cell_type":"code","source":"# Crear un nou model sense pesos previs\nlr=1e-5\nmodel3 = get_compiled_model(lr)\nprint(model3)\n\n# Entrenament del model\nhistory3 = model3.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true,"id":"mm3YFYpY3zZ6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history3)\n\n# Validaci贸 al conjunt de prova\nresult3 = model3.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n    <ul>\n        <li> <strong> Forma de les corbes d'entrenament </strong>: (escriu la teva resposta aqu铆)</li>\n        <li> <strong> N煤mero d猫poques/temps dentrenament </strong>: (escriu la teva resposta aqu铆)</li>        \n        <li> <strong> Rendiment del model al conjunt de prova</strong>: (escriu la teva resposta aqu铆)</li>\n        <li> <strong>Elecci贸 final </strong>: (escriu la teva resposta aqu铆)</li>\n    </ul>\n\n<strong>Refer猫ncies</strong>:\n    <ul>\n        <li>https://keras.io/api/callbacks/</li>\n        <li>https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights</li>\n        <li>https://keras.io/guides/training_with_built_in_methods/</li>\n        <li>https://keras.io/api/models/model_training_apis/</li>\n        <li>https://keras.io/guides/writing_your_own_callbacks/</li>\n    </ul>\n</div>","metadata":{"id":"FccpVTAE3zZ7"}},{"cell_type":"markdown","source":"## 3. Petita xarxa convolucional (2 punts)\n\nAra implementarem una xarxa neuronal convolucional bsica (CNN), que sol ser molt m茅s efica莽 per a la classificaci贸 de les imatges. Les CNN aprofiten l'estructura espacial de les dades mitjan莽ant capes espec铆fiques que permeten extreure les caracter铆stiques locals abans de la classificaci贸.\n\n**Arquitectura proposada**: Utilitzarem un model de Keras Sequential per a aquesta CNN. Consistir en un bloc dextractor de caracter铆stiques i, a continuaci贸, un classificador dens similar a l'anterior per貌 m茅s petit.\n\nEn aquesta secci贸 utilitzarem les capes  [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/),  [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/), [Dense](https://keras.io/api/layers/core_layers/dense/) i [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) de Keras.\n\nEs proporciona el codi del classificador ja implementat.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.1 [0,5 pts]:</strong> Analitza el codi seg眉ent i descriu les capes que formen el bloc extractor de caracter铆stiques i el classificador.</div>","metadata":{"id":"Sh7d7lG_jTlz"}},{"cell_type":"code","source":"# Definici贸 del model CNN\ncnn_model = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n], name=\"CNN_model\")\ncnn_model.summary()","metadata":{"trusted":true,"id":"GRjz2My8jTl0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Resposta: </strong>\n<br>\n <ul>\n     <li> <u>Bloc convolucional:</u> \n         <br>La primera capa normalitza els valors dels 3 canals a valors entre 0 i 1. \n         <br>La primera capa convolucional aplica 16 filtres de 3x3 en 3 canals amb s=1 i padding=1 (nombre de parmetres = (3x3x3+1)x16 = 448), aplica la funci贸 d'activaci贸 ReLU i redueix la dimensionalitat a la meitat amb un filtre max_pooling de 2x2. \n         <br>La segona capa convolucional aplica 32 filtres de 3x3 en 16 canals amb s=1 i padding=1 (nombre de parmetres = (3x3x16+1)x32 = 4640), aplica la funci贸 d'activaci贸 ReLU i redueix la dimensionalitat a la meitat amb un filtre max_pooling de 2x2.  \n         <br>La tercera capa convolucional aplica 64 filtres de 3x3 en 32 canals amb s=1 i padding=1 (nombre de parmetres = (3x3x32+1)x64 = 18496), aplica la funci贸 d'activaci贸 ReLU i redueix la dimensionalitat a la meitat amb un filtre max_pooling de 2x2. \n     </li>\n     <li> <u>Classificador final:</u>\n         <br>La primera capa FC inclou una capa dropout per tal de prevenir el sobreentrenament que desactiva un 20% de les entrades (nom茅s activa en la fase d'entrenament) i una capa densa de 64 neurones actuant sobre els 64 canals (nombre de parmetres = (64+1)*64 = 4160).\n         <br>La segona capa FC inclou una capa dropout per tal de prevenir el sobreentrenament que desactiva un 50% de les entrades (nom茅s activa en la fase d'entrenament) i una capa densa de 10 neurones actuant sobre els 64 canals (nombre de parmetres = (64+1)*10 = 650) activada amb 'softmax' per tal de donar-nos la sortida del classificador.\n     </li>\n <ul>\n</div>","metadata":{"id":"v8QJO2e5jTl0"}},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.2 [0.5 pts]</strong>: Compila el model amb l'optimitzador <strong>Adam</strong> i <em>learning rate</em> = 1e-4. Entrena el model durant <strong>100 猫poques</strong>, utilitzant:\n    <ul>\n        <li><em>EarlyStopping</em> monitoritzant la p猫rdua (<em>loss</em>) en validaci贸, amb paci猫ncia de 10 猫poques, restaurant els millors pesos obtinguts.</li>\n        <li><em>ReduceLROnPlateau</em> monitoritzant la p猫rdua (<em>loss</em>) en validaci贸, factor 0.2, paci猫ncia de 5 猫poques i un learning rate m铆nim de 1e-6</i>\n        <li>La m猫trica de rendiment <em>accuracy</em> tant en entrenament com en validaci贸.</li>\n    </ul>\n\nMostra una grfica de les corbes d'accuracy i loss per als conjunts d'entrenament i validaci贸.\n\nFinalment, avalua el model entrenat sobre el conjunt de <test>test</test>, mostrant la p猫rdua i exactitud (<it>accuracy</it>) final obtingudes en aquestes dades.\n\n<strong>Nota: durant l'execuci贸 del codi es produir un error.</strong>\n\nExplica a qu猫 茅s degut l'error en l'execuci贸 anterior i modifica el model per solucionar-lo.\n\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"CkP9wfgF3zaG"}},{"cell_type":"code","source":"# Definici贸 funci贸 compilaci贸\ndef get_compiled_model_cnn(lr,ref_model):\n    model = keras.models.clone_model(ref_model)\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"id":"GlgOsDS9jTl0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilar model CNN\nlr=1e-4\nmodel4 = get_compiled_model_cnn(lr,cnn_model)\nmodel4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Entrenament del model CNN\n# history4 = model4.fit(\n#     train_ds,\n#     validation_data=val_ds,\n#     epochs=100,\n#     batch_size=batch_size,\n#     callbacks=[early_stopping,reduce_lr,custom_callback],\n#     verbose=1\n# )\n\n# # Grafics d'accuracy i loss\n# get_acc_loss_plot(lr,history4)\n\n# # Validaci贸 al conjunt de prova\n# result4 = model4.evaluate(test_ds)","metadata":{"trusted":true,"id":"DGrNTcJijTl0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<br>\n<br>\nL'error es deu a qu猫 les dimensions output i target no s贸n iguals ja que no s'ha aplanat la sortida.\n    <br> <br>\n<strong>Refer猫ncies</strong>:\n    <ul>\n        <li>https://keras.io/api/layers/convolution_layers/convolution2d/</li>\n        <li>https://keras.io/api/layers/pooling_layers/max_pooling2d/</li>\n        <li>https://keras.io/api/layers/regularization_layers/dropout/</li>\n        <li>https://keras.io/guides/serialization_and_saving/</li>\n    </ul>\n</div>","metadata":{"id":"dKiBjmms3zaH"}},{"cell_type":"code","source":"# Escriu aqu铆 el model corregit\ncnn_model_c = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Flatten(),\n    Dense(10, activation='softmax')\n])\ncnn_model_c.summary()","metadata":{"trusted":true,"id":"MSeN6pUQ3zaH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.3 [0.5 pt]:</strong> Torna a repetir l'exercici anterior (entrenament, mostrar grfiques i validaci贸 en el conjunt de test) amb el model corregit. Comenta la mida del model, el nombre d'epochs/temps d'entrenament, les corbes d'entrenament i els resultats de validaci贸.\n<br><br>\n<strong>NOTA: Es valorar la concisi贸 en la resposta a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).</strong>\n\n</div>","metadata":{"id":"TZN8ZypU3zaI"}},{"cell_type":"code","source":"# Compilar model CNN\nlr=1e-4\nmodel5 = get_compiled_model_cnn(lr,cnn_model_c)\nmodel5","metadata":{"trusted":true,"id":"ktByKLKB3zaI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament del model CNN\nhistory5 = model5.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr],\n    verbose=1\n)","metadata":{"trusted":true,"id":"IdcQLyLs3zaJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history5)\n\n# Validaci贸 al conjunt de prova\nresult5 = model5.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Escriu aqu铆 el model corregit\ncnn_model_d = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\ncnn_model_d.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilar model CNN\nlr=1e-4\nmodel6 = get_compiled_model_cnn(lr,cnn_model_d)\n\n# Entrenament del model CNN\nhistory6 = model6.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history6)\n\n# Validaci贸 al conjunt de prova\nresult6 = model6.evaluate(test_ds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>cnn_model_c:</strong>\n<br>El model t茅 una mida de: 283.754 parmetres.\n<br>El nombre d'epochs d'entrenament ha estat de xxx amb un temps de xxx, les corbes d'entrenament i els resultats de validaci贸.\n<br> <br>\n<strong>cnn_model_d:</strong>\n<br>El model t茅 una mida de: 1.662.698 parmetres.\n<br>El nombre d'epochs d'entrenament ha estat de xxx amb un temps de xxx, les corbes d'entrenament i els resultats de validaci贸.\n<br> <br>\n</div>","metadata":{"id":"-wT_GLeL3zaJ"}},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 3.4 [0.5 pt]:</strong> Realitza una comparativa entre els models dels exercicis 2 (amb el <em>learning rate</em> escollit) i 3 en termes de:\n<ul>\n  <li> <strong> Mida del model </strong> </li>\n  <li> <strong> Evoluci贸 de l'entrenament </strong> </li>\n  <li> <strong> Validaci贸 en el conjunt de prova </strong> </li>\n  <li> <strong> Temps d'execuci贸 </strong> </li>\n</ul>\n\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).\n\n</div>","metadata":{"id":"9zqrLKeD3zaK"}},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong>Comentaris:</strong>\n<ul>\n  <li> <strong>Mida del model</strong>: (escriu la teva resposta aqu铆)</li>\n  <li> <strong>Evoluci贸 de l'entrenament</strong>: (escriu la teva resposta aqu铆)</li>\n  <li> <strong>Validaci贸 en el conjunt de prova</strong>: (escriu la teva resposta aqu铆)</li>\n  <li> <strong>Temps d'execuci贸</strong>: (escriu la teva resposta aqu铆)</li>\n</ul>\n</div>","metadata":{"id":"jpeJpPyj3zaL"}},{"cell_type":"markdown","source":"## 4. Augmentaci贸 de dades (1 punt)\n\nTot i que la CNN entrenada 茅s for莽a efectiva, podem intentar millorar-ne la generalitzaci贸 augmentant artificialment la mida i la diversitat del conjunt d'entrenament mitjan莽ant **t猫cniques d'augmentaci贸 de dades**. L'augmentaci贸 consisteix a aplicar transformacions aleat貌ries a les imatges (girs, rotacions, zoom, etc.) de manera que el model rebi variants de les imatges originals a cada 猫poca, simulant tenir m茅s dades\n\nKeras proporciona capes de preprocessament d'imatge que realitzen aquestes transformacions de manera eficient durant l'entrenament, per exemple RandomFlip, RandomRotation, RandomZoom entre d'altres.\n\nAqu铆 farem servir algunes d'aquestes capes per implementar l'augment. En particular, provarem de voltejar horitzontalment les imatges i aplicar petites rotacions aleat貌ries.\n\n### 4.1. Definici贸 i visualitzaci贸 de l'augmentaci贸\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 4.1 [0,5 pts]:</strong> Crea un model <strong>Sequential</strong> de Keras que representi una capa d'augmentaci贸 de dades amb les seg眉ents capes:\n  <ul>\n      <li><code>RandomFlip</code> que voltegi aleat貌riament les imatges horitzontalment.</li>\n      <li><code>RandomRotation</code> amb factor de rotaci贸 de 0.1 (卤10%).</li>\n  </ul>\nSelecciona una imatge del conjunt d'entrenament i passa-la a trav茅s d'aquest model diverses vegades, mostrant les imatges resultants per comprovar visualment les transformacions realitzades (voltejos i rotacions).\n</div>","metadata":{"id":"4YFqSk28jTl0"}},{"cell_type":"code","source":"# Model d'augmentaci贸 de dades\nmodel_aug = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    RandomFlip(mode='horizontal'),\n    RandomRotation(0.1)\n])","metadata":{"trusted":true,"id":"ziTG9gAdjTl4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prendre un batch d'entrenament i obtenir una imatge\nrd_class_path = os.path.join(train_path,random.choice(os.listdir(train_path)))\nrd_image_path = os.path.join(rd_class_path,random.choice(os.listdir(rd_class_path)))\n\n# Crrega de la imatge i conversi贸 a tensor\nimg = keras.preprocessing.image.load_img(rd_image_path, target_size= (img_height, img_width))\nimg_tensor = keras.preprocessing.image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\n\n# Normalitzem els valors al rang [0,1]\nimg_tensor /= 255.\n\n# Grafiquem la imatge\nplt.figure(figsize=(6,6))\nplt.imshow(img_tensor[0])\nplt.title('Imatge Original')\nplt.axis('off')\nplt.show()\n\n# Aplicar augmentaci贸 diverses vegades i visualitzar\nplt.figure(figsize=(12, 12))\nfor i in range(9):\n    augmented_img = model_aug(img_tensor, training=True)\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_img[0].numpy())\n    plt.axis(\"off\")\nplt.suptitle('Imatges Augmentades')\nplt.show()","metadata":{"trusted":true,"id":"FWJoHXhujTl5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2. Entrenament de la CNN amb augmentaci贸\n\nAra incorporarem la capa daugmentaci贸 al model CNN per entrenar-la amb dades augmentades a cada 猫poca.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 4.2 [0,5 pts]:</strong> Insereix el model d'augmentaci贸 de dades creat entre la capa de <code>Rescaling</code> i la primera <code>Conv2D</code> del model CNN anterior. s a dir, modifica l'arquitectura perqu猫 les imatges d'entrada, despr茅s de ser reescalades, passin per les capes de <em>flip</em> i <em>rotation</em> aleat貌ries, i despr茅s continu茂n per la CNN. A continuaci贸, compila i entrena el nou model CNN augmentat seguint les mateixes indicacions que en l'exercici anterior, <strong>excepte</strong> que aquesta vegada farem servir un <strong>learning rate</strong> inicial m茅s gran (1e-3). Mantingues EarlyStopping, ReduceLROnPlateau, 100 猫poques, etc. Despr茅s avalua el model final al conjunt de test.\n\nComenta les difer猫ncies amb el model sense augmentaci贸 en termes de:\n\n<ul>\n    <li> <strong> Evoluci贸 de les corbes d'entrenament </strong> </li>\n    <li> <strong> Validaci贸 en el conjunt de prova </strong> </li>\n    <li> <strong> Temps d'entrenament </strong> </li>\n    <li> <strong> Learning rate </strong> </li>\n</ul>\n\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).\n\n</div>","metadata":{"id":"m732VSK1jTl5"}},{"cell_type":"code","source":"# Model CNN amb augmentaci贸 de dades\ncnn_model_aug = keras.Sequential([\n    keras.layers.InputLayer(shape=(img_height, img_width, 3)),\n    Rescaling(1./255),\n    RandomFlip(mode='horizontal'),\n    RandomRotation(0.1),\n    Conv2D(16, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\ncnn_model_aug.summary()","metadata":{"trusted":true,"id":"RQIGxy4WjTl5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilaci贸 de la xarxa\nlr=1e-3\nmodel7 = get_compiled_model_cnn(lr,cnn_model_aug)","metadata":{"trusted":true,"id":"iqRmCU2DjTl5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament del model CNN\nhistory7 = model7.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n    batch_size=batch_size,\n    callbacks=[early_stopping,reduce_lr,custom_callback],\n    verbose=1\n)","metadata":{"trusted":true,"id":"sKXAr1DUjTl5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resultats\n# Grafics d'accuracy i loss\nget_acc_loss_plot(lr,history6)\n\n# Validaci贸 al conjunt de prova\nresult7 = model7.evaluate(test_ds)","metadata":{"trusted":true,"id":"VP3Q9ArDjTl6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n\n<ul>\n  <li> <strong> Evoluci贸 de les corbes d'entrenament</strong>: (escriu la teva resposta aqu铆) </li>\n  <li> <strong> Validaci贸 en el conjunt de prova</strong>: (escriu la teva resposta aqu铆) </li>\n  <li> <strong> Temps d'entrenament</strong>: (escriu la teva resposta aqu铆) </li>\n  <li> <strong> Learning rate</strong>: (escriu la teva resposta aqu铆) </li>\n</ul>\n<br>\n<strong> Refer猫ncies: </strong>\n<br> <br>\n\n<ul>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/</li>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_augmentation/</li>\n    <li>https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/</li>\n    <li>https://gac6.medium.com/visualizing-data-augmentations-from-keras-image-data-generator-44f040aa4c9f</li>\n<li>chatgpt.com</li>\n</ul>\n\n</div>","metadata":{"id":"3GX8JzB9jTl6"}},{"cell_type":"markdown","source":"## 5. Superresoluci贸 dimatges amb CNNs (2 pts)\n\nFins ara hem treballat en un problema de classificaci贸. Als apartats restants abordarem un problema diferent per貌 relacionat amb la visi贸 per computador: la **superresoluci贸 d'imatges**. La superresoluci贸 consisteix a generar una imatge d'alta resoluci贸 (HR) a partir d'una de baixa resoluci贸 (LR), intentant recuperar o inferir els detalls perduts en reduir la imatge. s un problema d'aprenentatge supervisat on el model apr猫n una transformaci贸 imatge->imatge.\n\nFarem servir novament la base de dades Imagenette per crear exemples d'entrenament: a partir de cada imatge original (320px) generarem una versi贸 redu茂da (p. ex. 80px) que servir com a entrada, tenint com a sortida esperada la imatge original. Aix铆, el model aprendr a mapejar de baixa a alta resoluci贸. En lloc d'una xarxa convolucional per a la classificaci贸, necessitarem una xarxa capa莽 de processar una imatge d'entrada i produir una imatge de sortida. Les **capes transposades de convoluci贸** ([Conv2DTranspose](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/)) o les t猫cniques de upsampling s贸n les peces clau per a aquests models, ja que permeten augmentar les dimensions espacials de les dades.\n\nA continuaci贸, crearem el conjunt de dades per a superresoluci贸 i entrenarem una **CNN de superresoluci贸 simple**.\n\n### 5.1. Preparaci贸 de dades de LR/HR\n\nPrimer generarem els parells d'imatges d'entrenament i de validaci贸 per a superresoluci贸. Partirem de les imatges originals d'entrenament (i validaci贸) a la seva resoluci贸 completa i, per simplicitat, les redimensionarem a una mida fixa de 320320 (ignorant la relaci贸 d'aspecte original, similar al fet en classificaci贸) per utilitzar-les com a imatges de refer猫ncia d'alta resoluci贸 (HR), i les reduirem a 1/4 de la seva mida (aprox). Per realitzar ambdues transformacions (ajust de les imatges originals a 320x320 i la seva reducci贸 a 80x80) utilitzarem m猫todes d'[interpolaci贸 bic煤bica](https://es.wikipedia.org/wiki/Interpolaci%C3%B3n_bic%C3%BAbica) per simular imatges degrades suaument.\n\n\nProcedim a obtenir les rutes d'imatges d'entrenament i validaci贸, despr茅s creem un Dataset aplicant la funci贸 de mapeig que realitza la lectura i la transformaci贸:\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 5.1.1 [0,5 pt]: </strong>El codi seg眉ent prepara un nou conjunt de dades a partir de les imatges d'entrenament originals. Cada exemple consistir en una imatge de baixa resoluci贸 (LR) i la corresponent imatge d'alta resoluci贸 (HR). Concretament, pren les imatges de <code>/train/</code> (per crear els subconjunts d'entrenament i validaci贸) a la seva mida completa (320px) com a HR, i genera imatges LR reduint-les a 1/4 de la seva mida lineal (8080). Es demana:\n<ul>\n    <li> Completa els comentaris del codi en aquells llocs on est marcat (# Posar Comentari).\n</ul>\n</div>","metadata":{"id":"SIPuOf5bjTl6"}},{"cell_type":"code","source":"# Funci贸 per al preprocessat d'imatges per al problema de SR\ndef preprocess_image_for_sr(filepath):\n    # Crrega i descodificaci贸 del fitxer\n    img = tf.io.read_file(filepath)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Conversi贸 a tensor float32 (valors entre 0 i 1)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # Reescalat mitjan莽ant interpolaci贸 bic煤bica\n    hr = tf.image.resize(img, [320, 320], method=tf.image.ResizeMethod.BICUBIC)\n    lr = tf.image.resize(img, [80, 80], method=tf.image.ResizeMethod.BICUBIC)\n    # Normalitzaci贸 dels valors del tensor\n    hr = tf.clip_by_value(hr, 0.0, 1.0)\n    lr = tf.clip_by_value(lr, 0.0, 1.0)\n    return lr, hr\n\n# Obtenim els conjunts d'entrenament i validaci贸\ntrain_files = []\nval_files = []\nfor cls in train_classes:\n    cls_files = random.shuffle((train_path/cls).glob(\"*.*\"))\n    # Separem els conjunts d'entrenament (80%) i validaci贸 (20%)\n    split_idx = int(len(cls_files) * 0.2)\n    val_files += [str(p) for p in cls_files[:split_idx]]\n    train_files += [str(p) for p in cls_files[split_idx:]]\nrandom.shuffle(train_files)\nrandom.shuffle(val_files)\n\n# Creem els datasets d'entrenament i validaci贸\ntrain_sr_ds = tf.data.Dataset.from_tensor_slices(train_files) # Rutes\n.map(preprocess_image_for_sr, num_parallel_calls=tf.data.AUTOTUNE) # LR, HR\n.batch(32) # Agrupaci贸 lots de 32\n.prefetch(tf.data.AUTOTUNE) # Optimitzaci贸 processament\nval_sr_ds   = tf.data.Dataset.from_tensor_slices(val_files)\n.map(preprocess_image_for_sr, num_parallel_calls=tf.data.AUTOTUNE)\n.batch(32)\n.prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"id":"MxjJviEfjTl7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Procedim a verificar la base de dades:","metadata":{"id":"1udwGw3K3zaR"}},{"cell_type":"code","source":"# Verificaci贸 d'un parell LR-HR\nfor lr_batch, hr_batch in train_sr_ds.take(1):\n    lr_img = lr_batch[0].numpy()\n    hr_img = hr_batch[0].numpy()\n    print(\"LR shape:\", lr_img.shape, \"HR shape:\", hr_img.shape)\n    print(\"LR pixel range:\", lr_img.min(), \"-\", lr_img.max())\n    print(\"HR pixel range:\", hr_img.min(), \"-\", hr_img.max())\n    break\nprint(f\"El nombre d'imatges en el conjunt d'entrenament 茅s {len(train_files)}\")\nprint(f\"El nombre d'imatges en el conjunt de validaci贸 茅s {len(val_files)}\")","metadata":{"trusted":true,"id":"VRFA2ufvjTl7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style = \"background-color: #edf7ff; border-color: #7c9dbf; frontera-esquerra: 5px s貌lid #7c9dbf; encoixinat: 0,5EM;\"> <strong> Exercici 5.1.2 [0,5 pt]: </strong> Respon a les preguntes seg眉ents:\n<ul>\n <li>A qu猫 es pot deure l'煤s de la funci贸 'clip_by_value' al codi anterior?</li>\n <li>Com es divideix la base de dades? Quin directori s'utilitza per crear-la? Quins subconjunts es creen? Quin percentatge d'imatges s'assigna a cada conjunt? I en nombre d'imatges?</li>\n <li>Per a cada element del conjunt de dades, quin format tenen les dades que farem servir per entrenar el model? i les etiquetes?</li>\n</ul>\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"DcNsu5Zd3zaS"}},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n <ul>\n    <li> <strong> Funci贸 'clip_by_value' </strong>: La funci贸 ens permet corregir possibles valors fora del rang [0,1] generats en la interpolaci贸 bic煤bica. </li>\n    <li> <strong> Subconjunts de la base de dades </strong>: La base de dades es divideix en un conjunt d'entrenament que cont茅 un 80% de les dades (7578 imatges) i un conjunt de validaci贸 que cont茅 el 20% restant (1891 imatges). </li>\n    <li> <strong> Format de les dades i etiquetes </strong>: Les dades d'entrada (LR) tenen una mida de 80x80 p铆xels representats en un tensor (80,80,3). Les etiquetes (HR) tenen una mida de 320x320 p铆xels representats en un tensor (320,320,3) </li>\n</ul>\n<strong> Refer猫ncies: </strong>\n<br> <br>\n <ul>\n     <li>https://www.tensorflow.org/api_docs/python/tf/clip_by_value</li>\n     <li>chatgpt.com</li>\n</ul>\n</div>","metadata":{"id":"gkOrAEFsjTl8"}},{"cell_type":"markdown","source":"### 5.2. Implementaci贸 del model de Superresoluci贸\n\nAra definim un model CNN per a Superresoluci贸. Optarem per una arquitectura senzilla amb [UpSampling2D](https://keras.io/api/layers/reshaping_layers/up_sampling2d/) per escalar la imatge gradualment. Implementem el model:\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 5.2 [1 pt]: </strong> Entrenarem un model CNN per superresoluci贸 4 (de 80px a 320px). Per fer aix貌:\n    <ul>\n        <li> Implementa un model de superresoluci贸 a Keras. Podeu utilitzar lAPI funcional o Sequential. Suggeriment d'arquitectura simple:\n            <ul>\n                <li> Entrada amb forma (80,80,3).\n                <li> Diverses capes Conv2D per extreure les caracter铆stiques de les imatges de baixa resoluci贸.\n                <li> Capa de <code>Upsampling2D</code> (factor=2) o <code>Conv2DTranspose</code> amb <code>stride=2</code> per doblar l'amplada i al莽ada (80 -> 160).\n                <li> Una altra capa Conv2D per refinar, seguida d'una altra capa upsampling 2 (160 -> 320).\n                <li> Capes finals Conv2D per reconstruir la imatge de sortida 3203203. (Podeu utilitzar l'activaci贸 lineal o RELU per a la sortida; considereu que busquem valors de p铆xel entre 0-1 o 0-255).\n            </ul> </li>\n        <li> Compileu el model amb una funci贸 de p猫rdua adequada per a imatges (per exemple, MSE) i sense m猫triques de precisi贸. Entreneu-lo durant ~ 50-100 epochs, amb EarlyStopping (paci猫ncia 5-10, monitoritzaci贸 de val_loss) per assegurar-vos que no sobreentreneu.\n        <li> Comentar els resultats de l'entrenament. Concretament:\n            <ul>\n                <li> Corbes d'entrenament.\n                <li> El temps d'entrenament vs Tamany del model.\n            </ul>                \n        </li>\n    </ul>\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).\n</div>","metadata":{"id":"S-PYhtzP3zaT"}},{"cell_type":"code","source":"# Model de Superrsoluci贸 (80px -> 320px)\n# Model de Superresoluci贸 (80px -> 320px)\ninputs = keras.Input(shape=(80, 80, 3))\n\n# Extracci贸 de caracter铆stiques\nx = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\nx = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n\n# Primera fase d'upsampling (80 -> 160)\nx = layers.UpSampling2D(size=2)(x)\nx = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n\n# Segona fase d'upsampling (160 -> 320)\nx = layers.UpSampling2D(size=2)(x)\nx = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n\n# Refinament i reconstrucci贸\nx = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\noutputs = layers.Conv2D(3, 3, padding='same', activation='sigmoid')(x)  # Sortida entre 0-1","metadata":{"id":"8Ifu1K1Z7gVi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compilaci贸 de la xarxa\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer='adam', loss='mse')\n\n# Resum del model\nmodel.summary()","metadata":{"trusted":true,"id":"s7qM9vifjTl7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenament de la xarxa\n","metadata":{"trusted":true,"id":"-qEbXQxEjTl7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resultats\n","metadata":{"trusted":true,"id":"4d4o0XQ4jTl7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n<ul>\n    <li> <strong> Corbes dentrenament </strong>: (escriviu la vostra resposta aqu铆) </li>\n    <li> <strong> Temps d'entrenament vs Tamany del model </strong>: (escriviu la vostra resposta aqu铆) </li>\n</ul>\n</div>","metadata":{"id":"9Kmeo87v3zaW"}},{"cell_type":"markdown","source":"## 6. Infer猫ncia, m猫triques i visualitzaci贸 dels resultats (1 punt)\n\nAmb el model de superresoluci贸 entrenat, avaluarem el seu rendiment tant quantitativament (amb m猫triques) com qualitativament (visualitzaci贸 dimatges). Per fer -ho, realitzarem la infer猫ncia: 茅s a dir, prendre imatges LR del conjunt de test, passar-les pel model <code>sr_model</code> per obtenir imatges supers resoltes (SR) i comparar-les amb les imatges HR originals. Calcularem la m猫trica [PSNR (Peak Signal-to-Notise Ratio)](https://es.wikipedia.org/wiki/psnr) per quantificar la qualitat.\n\nEl PSNR es mesura en decibels (dB) i els valors superiors indiquen una major similitud amb la imatge original (per exemple,> 30 dB sol indicar una qualitat de reconstrucci贸 molt bona).\n\nTamb茅 visualitzarem alguns exemples, on mostraren la imatge de baixa resoluci贸 (LR), la super resolta per la CNN (SR) i loriginal dalta resoluci贸 (HR), per inspeccionar els detalls amb lull nu.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong>Exercici 6 [1 pt]:</strong> Realitza la infer猫ncia del model de superresoluci贸 sobre diverses imatges de test:\n<ul>\n  <li>Per a cada imatge de test, obt茅 la seva versi贸 de baixa resoluci贸 (LR) i genera la imatge superresolta (SR) amb el model.</li>\n  <li>Calcula la m猫trica PSNR comparant la imatge SR amb la HR original per a tot el conjunt de test.</li>\n  <li>Mostra visualment alguns exemples, incloent-hi la imatge d'entrada LR (pots ampliar-la per mostrar-la de la mateixa mida utilitzant el m猫tode <code>tf.image.resize()</code> juntament amb <code>method=tf.image.ResizeMethod.NEAREST_NEIGHBOR</code>), la imatge generada pel model (SR) i la imatge HR original, per poder comparar la qualitat visualment a part del valor de la PSNR.</li>\n</ul>\nComentar els resultats obtinguts:\n<ul>\n    <li> Quins valors de PSNR sobtenen? (Comenta tant els exemples mostrats com el valor mitj del total del conjunt de prova) </li>\n    <li> Les imatges creades amb superresoluci贸 s贸n n铆tides o borroses?\n    <li> Quins tipus de detalls o errors sobserven?\n</ul>\n<strong> Nota: es valorar la concisi贸 a les preguntes (una o dues frases s贸n suficients per a cadascuna de les respostes).  \n</div>","metadata":{"id":"piSdBJmP8f81"}},{"cell_type":"code","source":"# Preparar conjunt de test per a SR (a partir del directori /val )\n","metadata":{"trusted":true,"id":"4hqRsPE_3zaY"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Llista per emmagatzemar la PSNR de cada imatge\n\n# Rec贸rrer el dataset de test (batch size = 1)\n\n # Generar la imatge de superresoluci贸 a partir de la imatge LR\n\n # Treure la dimensi贸 de batch i assegurar que els valors estiguin a [0,1]\n\n # Convertir la imatge HR a array de numpy (llevem la dimensi贸 de batch)\n\n # Calcular la PSNR entre la imatge SR generada i la imatge HR original\n\n # Emmagatzemar el valor de PSNR a la llista\n\n\n# Calcular la PSNR mitjana del conjunt de test\navg_psnr =\nprint(\"PSNR mitjana al conjunt de test: {:.2f} dB\".format(avg_psnr))","metadata":{"trusted":true,"id":"WR3mXWV33zaZ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\n# Escollir algunes imatges de test aleat貌ries per a visualitzaci贸\n\n# Per a cada imatge\n\n # Llegir i processar una imatge de test\n\n # Generar superresoluci贸\n\n # Calcular PSNR\n\n # Preparar imatges per visualitzar de la mateixa mida\n\n # Les imatges estan a [0,1], escalar a [0,255] per mostrar correctament\n\n # Mostrar les imatges\n\n","metadata":{"trusted":true,"id":"9gpUyeINjTl-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\n   Anlisi dels resultats del model SR:\n   <ul>\n       <li> <strong> PSNR </strong>: (escriviu la vostra resposta aqu铆) </li>\n       <li> <strong> Qualitat visual </strong>: (escriviu la vostra resposta aqu铆) </li>\n       <li> <strong> Errors </strong>: (escriviu la vostra resposta aqu铆) </li>\n   </ul>\n        \n   (Escriviu la vostra resposta aqu铆)\n</div>","metadata":{"id":"UVHiHLuSjTl-"}},{"cell_type":"markdown","source":"## 7. Model pre-entrenat en superresoluci贸 (1 punt)\n\nEls aven莽os recents en superresoluci贸 han produ茂t arquitectures m茅s complexes (p. ex., basades en xarxes generatives adversries) que aconsegueixen resultats notablement millors, a costa d'un entrenament cost贸s. Un d'aquests models 茅s ESRGAN ([Enhanced Super-Resolution GAN per Xintao Wang et al.](https://arxiv.org/abs/1809.00219)), entrenat en grans bases de dades d'imatges HD (com DIV2K) per aconseguir superresoluci贸 4x de gran fidelitat.\n\nFarem servir un model pre-entrenat d'ESRGAN disponible via [TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/image_enhancing)\n\nAquest model ha estat entrenat al conjunt DIV2K (imatges d'alta qualitat) amb degradaci贸 bic煤bica, per la qual cosa est especialitzat en produir imatges 4 m茅s grans amb detall notable.\n\nFarem servir ESRGAN per aplicar superresoluci贸 a les mateixes imatges de test i compararem els resultats amb el nostre model implementat a l'exercici 5.\n\n<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 7 [1 pt]:</strong> Utilitza un model pre-entrenat de superresoluci贸 (p. ex. ESRGAN 4) per millorar les imatges de baixa resoluci贸. En concret:\n  <ul>\n      <li>Carrega el model pre-entrenat des de TensorFlow Hub (usa l'URL proporcionada, per exemple <code>\"https://tfhub.dev/captain-pool/esrgan-tf2/1\"</code>).</li>\n      <li>Per a una selecci贸 d'imatges de test de baixa resoluci贸 (8080), obteniu la superresoluci贸 4 amb aquest model.</li>\n      <li>Calcula la PSNR de les imatges generades pel model pre-entrenat (en el total del conjunt de test) i compara la seva mitjana amb la PSNR mitjana obtinguda pel teu model.</li>\n      <li>Visualitza les imatges de sortida del model pre-entrenat al costat de les del teu model i les originals, per comparar visualment la qualitat. Compara tamb茅 la PSNR d'aquestes imatges.</li>\n  </ul>\nDiscuteix les difer猫ncies observades:\n  <ul>\n      <li>El model pre-entrenat obt茅 millors m猫triques?</li>\n      <li>El model pre-entrenat produeix imatges m茅s n铆tides i properes a la realitat?</li>\n      <li>En quins detalls es nota la millora?</li>\n  </ul>\n</div>","metadata":{"id":"tJHyY2JjjTl_"}},{"cell_type":"code","source":"import tensorflow_hub as hub\n\n# Carreguem el model ESRGAN des de TF-Hub:\nesrgan = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\nprint(\"Model ESRGAN carregat.\")","metadata":{"trusted":true,"id":"9SiuIY5WjTl_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Llista per emmagatzemar la PSNR de cada imatge usant ESRGAN\n\n# Rec贸rrer el dataset de test (batch size = 1)\n\n # Preparar la imatge LR per ESRGAN:\n # ESRGAN espera entrada en rang [0,255] com a float32, i test_sr_ds t茅 imatges a [0,1]\n\n # Generar la imatge de superresoluci贸 amb ESRGAN\n\n # Treure la dimensi贸 de batch i normalitzar la sortida a [0,1]\n\n # Obtenir la imatge HR corresponent (llevem la dimensi贸 de batch)\n\n # Calcular la PSNR entre la imatge generada per ESRGAN i la imatge HR original\n\n\n# Calcular la PSNR mitjana en el conjunt de test amb ESRGAN\navg_psnr_esrgan =\nprint(\"PSNR mitjana en el conjunt de test amb ESRGAN: {:.2f} dB\".format(avg_psnr_esrgan))","metadata":{"trusted":true,"id":"HWdLPlYU3zaf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apliquem ESRGAN a les mateixes imatges d'exemple que fem servir amb el nostre model\n\n# Per a cada imatge\n\n # Preparar la imatge LR de test\n\n # Convertir sortida a [0,1] float\n\n # Calcular PSNR comparat amb HR\n\n # Visualitzar comparativa\n","metadata":{"id":"hyWzmyZc-Umu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<ul>\n    <li> <strong> M猫trica (PSNR) </strong>: (escriviu la vostra resposta aqu铆) </li>\n    <li> <strong> Qualitat visual </strong>: (escriviu la vostra resposta aqu铆) </li>\n    <li> <strong> Detalls </strong>: (escriviu la vostra resposta aqu铆) </li>\n</ul>  \n</div>","metadata":{"id":"Jn2JeQrUjTmA"}},{"cell_type":"markdown","source":"## 8. Comparaci贸 i conclusions (0,5 pts)\n\nEn aquesta prctica hem explorat tant la classificaci贸 dimatges amb xarxes neuronals (denses vs convolucionals) com la superresoluci贸 amb CNNs, aplicant-ho tot sobre la base de dades dImagenette.","metadata":{"id":"nX8PEeKp-znz"}},{"cell_type":"markdown","source":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> <strong> Exercici 8 [0,5 pts]: </strong> Fes un resum dels punts apresos en aquest PAC. Concretament, resumeix en una frase el que has apr猫s de cadascun dels punts enumerats a continuaci贸: <ul>\n    <li> <strong> ANN vs CNN: </strong> </li>\n    <li> <strong> Regularitzaci贸 i augmentaci贸: </strong> </li>\n    <li> <strong> Superresoluci贸: </strong> </li>\n    <li> <strong> Transfer learning: </strong> </li>\n</ul>\nValoraci贸 final\n</div>","metadata":{"id":"TtxSISOB3zag"}},{"cell_type":"markdown","source":"<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n<strong> Comentaris: </strong>\n<br> <br>\nConclusions finals:\n<ul>\n    <li>\n        <strong> ANN vs CNN: </strong> (escriviu la vostra resposta aqu铆)\n    </li>\n    <li>\n        <strong> Regularitzaci贸 i augmentaci贸: </strong> (escriviu la vostra resposta aqu铆)\n    </li>\n    <li>\n        <strong> Superresoluci贸: </strong> (escriviu la vostra resposta aqu铆)\n    </li>\n    <li>\n        <strong> Transfer Learning: </strong> (escriviu la vostra resposta aqu铆)\n    </li>\n</ul>\n\n\n  (Escriviu la vostra resposta aqu铆)\n<br> <br>\n</div>","metadata":{"id":"X_W9yTgs-0NA"}}]}